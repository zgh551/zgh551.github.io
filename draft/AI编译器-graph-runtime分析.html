<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;zgh551.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:true,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="[TOC] 1.概述 本文主要分析运行端代码  powervr_tvm-graph  2.RUN函数 函数void GraphRuntime::Run()主要由流水线执行判断、设备执行线程创建和流水线执行等部分组成，下面对每个部分的代码做简要分析。 2.1.流水线执行判断 enable_pipe_exe默认值为True，除非用户调用DisablePipeline函数失效流水线执行">
<meta property="og:type" content="website">
<meta property="og:title" content="AI编译器-graph_runtime分析">
<meta property="og:url" content="https://zgh551.github.io/draft/AI%E7%BC%96%E8%AF%91%E5%99%A8-graph-runtime%E5%88%86%E6%9E%90.html">
<meta property="og:site_name" content="Z">
<meta property="og:description" content="[TOC] 1.概述 本文主要分析运行端代码  powervr_tvm-graph  2.RUN函数 函数void GraphRuntime::Run()主要由流水线执行判断、设备执行线程创建和流水线执行等部分组成，下面对每个部分的代码做简要分析。 2.1.流水线执行判断 enable_pipe_exe默认值为True，除非用户调用DisablePipeline函数失效流水线执行">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/Zhu%20Guohua/Pictures/powervr_tvm-graph.png">
<meta property="og:image" content="c:/Users/Zhu%20Guohua/Pictures/powervr_tvm.png">
<meta property="og:image" content="d:/WorkSpace/github/HexoBlog/source/draft/doc/powervr_tvm-memory.png">
<meta property="article:published_time" content="2021-05-21T06:53:57.000Z">
<meta property="article:modified_time" content="2021-06-27T08:33:11.095Z">
<meta property="article:author" content="Henry">
<meta property="article:tag" content="zgh551">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:/Users/Zhu%20Guohua/Pictures/powervr_tvm-graph.png">


<link rel="canonical" href="https://zgh551.github.io/draft/AI%E7%BC%96%E8%AF%91%E5%99%A8-graph-runtime%E5%88%86%E6%9E%90">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;zgh551.github.io&#x2F;draft&#x2F;AI%E7%BC%96%E8%AF%91%E5%99%A8-graph-runtime%E5%88%86%E6%9E%90.html&quot;,&quot;path&quot;:&quot;draft&#x2F;AI编译器-graph-runtime分析.html&quot;,&quot;title&quot;:&quot;AI编译器-graph_runtime分析&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>AI编译器-graph_runtime分析 | Z
</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Z</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">天道酬勤 知行合一</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">1.概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#run%E5%87%BD%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">2.RUN函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%89%A7%E8%A1%8C%E5%88%A4%E6%96%AD"><span class="nav-number">2.1.</span> <span class="nav-text">2.1.流水线执行判断</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E9%87%8A%E6%94%BE"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.2.1. 节点释放</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B"><span class="nav-number">2.2.</span> <span class="nav-text">2.2.创建线程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nna%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1. NNA执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nnpu%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2. NNPU执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cpu%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3. CPU执行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%89%A7%E8%A1%8C"><span class="nav-number">2.3.</span> <span class="nav-text">2.3.流水线执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1.获取输入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%87%86%E5%A4%87%E5%88%97%E8%A1%A8"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2. 更新准备列表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6%E5%87%86%E5%A4%87%E5%88%97%E8%A1%A8"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3. 调度准备列表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8E%E5%A4%84%E7%90%86"><span class="nav-number">2.3.4.</span> <span class="nav-text">2.3.4. 后处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#init%E5%87%BD%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">3. Init函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%AD%98%E5%82%A8"><span class="nav-number">3.1.</span> <span class="nav-text">3.1. 配置存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%89%A7%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="nav-number">3.2.</span> <span class="nav-text">3.2. 配置执行操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAtvm%E6%93%8D%E4%BD%9C%E7%AE%97%E5%AD%90"><span class="nav-number">3.3.</span> <span class="nav-text">3.3.创建TVM操作算子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#getfunction"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1. GetFunction</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#powervr_module"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">3.3.1.1 powervr_module</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Henry"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Henry</p>
  <div class="site-description" itemprop="description">Opportunity knocks but once</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">85</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zgh551" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zgh551" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zgh_email@163.com" title="E-Mail → mailto:zgh_email@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/zghforever" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;zghforever" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zgh551/" title="Linkindin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zgh551&#x2F;" rel="noopener" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>Linkindin</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/zgh551" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner page posts-expand">
  
  


    
    
    
    <div class="post-block" lang="zh-CN"><header class="post-header">

<h1 class="post-title" itemprop="name headline">AI编译器-graph_runtime分析
</h1>

<div class="post-meta-container">
</div>

</header>

      
      
      <div class="post-body">
          <p>[TOC]</p>
<h2 id="概述">1.概述</h2>
<p>本文主要分析运行端代码</p>
<figure>
<img src="C:\Users\Zhu%20Guohua\Pictures\powervr_tvm-graph.png" alt="powervr_tvm-graph" /><figcaption aria-hidden="true">powervr_tvm-graph</figcaption>
</figure>
<h2 id="run函数">2.RUN函数</h2>
<p>函数<code>void GraphRuntime::Run()</code>主要由流水线执行判断、设备执行线程创建和流水线执行等部分组成，下面对每个部分的代码做简要分析。</p>
<h3 id="流水线执行判断">2.1.流水线执行判断</h3>
<p><code>enable_pipe_exe</code>默认值为<code>True</code>，除非用户调用<code>DisablePipeline</code>函数失效流水线执行，否则不执行下述代码。如果不进行流水线执行方式，则依次执行节点数组中的节点操作并释放节点，最后直接退出运行。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (enable_pipe_exe == <span class="literal">false</span>) &#123;</span><br><span class="line">    <span class="comment">// setup the array and requirements.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; op_execs_.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (op_execs_[i]) &#123;</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[i];</span><br><span class="line">                <span class="keyword">const</span> TVMOpParam&amp; param = inode.param;</span><br><span class="line">                <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Executing node &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; <span class="built_in">GetNodeName</span>(i) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; param.func_name;</span><br><span class="line">            &#125;</span><br><span class="line">            op_execs_[i]();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//释放该节点，保存节点数据</span></span><br><span class="line">        <span class="built_in">DumpNode</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="节点释放">2.2.1. 节点释放</h4>
<p>如果设备非CPU,将设备的输入缓存区内容拷贝到CPU内存区域。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::DumpNode</span><span class="params">(<span class="keyword">size_t</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> is_param = (op_execs_[index] == <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[index];</span><br><span class="line">    <span class="keyword">const</span> TVMOpParam&amp; param = inode.param;</span><br><span class="line">    <span class="keyword">uint32_t</span> num_outputs = is_param ? <span class="number">1</span> : inode.param.num_outputs;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> o = <span class="number">0</span>; o &lt; num_outputs; ++o) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(index, o);</span><br><span class="line">        std::string fname = param.dump_name;</span><br><span class="line">        NDArray data = data_entry_[eid];</span><br><span class="line">        <span class="keyword">if</span> (!data.<span class="built_in">defined</span>() || fname == <span class="string">&quot;None&quot;</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        DLTensor *in = <span class="keyword">const_cast</span>&lt;DLTensor*&gt;(data.<span class="keyword">operator</span>-&gt;());</span><br><span class="line">        std::ostringstream dump_name_os;</span><br><span class="line">        <span class="function">std::string <span class="title">dname</span><span class="params">(inode.name)</span></span>;</span><br><span class="line">        std::<span class="built_in">replace</span>(dname.<span class="built_in">begin</span>(), dname.<span class="built_in">end</span>(), <span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;_&#x27;</span>);</span><br><span class="line">        dump_name_os &lt;&lt; index &lt;&lt; <span class="string">&quot;_&quot;</span> &lt;&lt; dname &lt;&lt; <span class="string">&quot;_&quot;</span>;</span><br><span class="line">        <span class="keyword">int</span> dim = in-&gt;ndim;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; ++i) &#123;</span><br><span class="line">            dump_name_os &lt;&lt; in-&gt;shape[i];</span><br><span class="line">            <span class="keyword">if</span> (i != (dim - <span class="number">1</span>)) dump_name_os &lt;&lt; <span class="string">&quot;x&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        dump_name_os &lt;&lt; <span class="string">&quot;_&quot;</span> &lt;&lt; tvm::runtime::<span class="built_in">TVMType2String</span>(in-&gt;dtype);</span><br><span class="line">        <span class="keyword">if</span> (o &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            dump_name_os &lt;&lt; <span class="string">&quot;_&quot;</span> &lt;&lt; o;    <span class="comment">/* when there are multiple outputs */</span></span><br><span class="line">        &#125;</span><br><span class="line">        fname = dump_name_os.<span class="built_in">str</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">void</span> *data_ptr = in-&gt;data;</span><br><span class="line">        <span class="keyword">size_t</span> size = <span class="built_in">GetDataSize</span>(*in);</span><br><span class="line">        <span class="keyword">bool</span> free_data = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Device of the node to dump &quot;</span> &lt;&lt; index &lt;&lt; <span class="string">&quot; is &quot;</span> &lt;&lt; in-&gt;ctx.device_type</span><br><span class="line">            &lt;&lt; <span class="string">&quot; Mem @&quot;</span> &lt;&lt; in-&gt;data &lt;&lt; <span class="string">&quot; Size &quot;</span> &lt;&lt; size &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; fname &lt;&lt; <span class="string">&quot; output_index &quot;</span> &lt;&lt; o;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (in-&gt;ctx.device_type != kDLCPU) &#123;</span><br><span class="line">            DLTensor to;</span><br><span class="line">            <span class="keyword">int</span> ret = <span class="built_in">posix_memalign</span>(&amp;data_ptr, <span class="number">64</span>, size);</span><br><span class="line">            <span class="keyword">if</span> (ret != <span class="number">0</span>) <span class="keyword">throw</span> std::<span class="built_in">bad_alloc</span>();</span><br><span class="line">            to.data = data_ptr;</span><br><span class="line">            to.ctx.device_type = kDLCPU;</span><br><span class="line">            to.ctx.device_id = <span class="number">0</span>;</span><br><span class="line">            to.ndim = in-&gt;ndim;</span><br><span class="line">            to.dtype = in-&gt;dtype;</span><br><span class="line">            to.shape = in-&gt;shape;</span><br><span class="line">            to.strides = in-&gt;strides;</span><br><span class="line">            to.byte_offset = in-&gt;byte_offset;</span><br><span class="line">            data_entry_[eid].<span class="built_in">CopyTo</span>(&amp;to);</span><br><span class="line">            free_data = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::fstream <span class="title">fp</span><span class="params">(fname, std::ios::out | std::ios::binary)</span></span>;</span><br><span class="line">        <span class="keyword">if</span> (fp.<span class="built_in">is_open</span>()) &#123;</span><br><span class="line">            fp.<span class="built_in">write</span>((<span class="keyword">const</span> <span class="keyword">char</span>*)data_ptr, size);</span><br><span class="line">            fp.<span class="built_in">close</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (free_data == <span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="built_in">free</span>(data_ptr);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="创建线程">2.2.创建线程</h3>
<p>分别为<code>nna</code>、<code>nnpu</code>和<code>cpu</code>创建线程变量，并根据设备上下文判定是否启动设备线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">std::thread nna_th;</span><br><span class="line">std::thread nnpu_th;</span><br><span class="line">std::thread cpu_th;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Join threads if the device context is not required */</span></span><br><span class="line"><span class="keyword">bool</span> need_nna = <span class="literal">false</span>, need_nnpu = <span class="literal">false</span>, need_cpu = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span>&amp; dev : attrs_.device_ctxs) &#123;</span><br><span class="line">    <span class="keyword">if</span> (dev == kDLPowerVRNNA) &#123;</span><br><span class="line">        need_nna = <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dev == kDLPowerVRNNPU ||</span><br><span class="line">               dev == kDLOpenCL ||</span><br><span class="line">               dev == kDLPowerVROpenCL) &#123;</span><br><span class="line">        need_nnpu = <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dev == kDLCPU ||</span><br><span class="line">               dev == kDLPowerVRPython ||</span><br><span class="line">               dev == kDLPowerVRCPP) &#123;</span><br><span class="line">        need_cpu = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于need_xxx变量，判定是否启动相应线程</span></span><br><span class="line"><span class="keyword">if</span> (need_nna == <span class="literal">true</span>) nna_th = std::<span class="built_in">thread</span>(&amp;GraphRuntime::ExecuteNNA, <span class="keyword">this</span>);</span><br><span class="line"><span class="keyword">if</span> (need_nnpu == <span class="literal">true</span>) nnpu_th = std::<span class="built_in">thread</span>(&amp;GraphRuntime::ExecuteNNPU, <span class="keyword">this</span>);</span><br><span class="line"><span class="keyword">if</span> (need_cpu == <span class="literal">true</span>) cpu_th = std::<span class="built_in">thread</span>(&amp;GraphRuntime::ExecuteCPU, <span class="keyword">this</span>);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>WaitForEvent</code>函数</li>
</ul>
<p>关于等待事件函数，主要根据设备类型执行不同的等待事件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">WaitForEvent</span><span class="params">(<span class="keyword">uint32_t</span> nid)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (_global_depend_map_.<span class="built_in">size</span>() == <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 获取依赖映射包</span></span><br><span class="line">    std::vector&lt;<span class="keyword">uint32_t</span>&gt; depends = _global_depend_map_[nid];</span><br><span class="line">    <span class="comment">// 定义函数指针类型</span></span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*t_opencl_wait)</span><span class="params">(<span class="keyword">uint32_t</span>, <span class="keyword">const</span> <span class="keyword">void</span>*)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">uint32_t</span> <span class="params">(*t_imgdnn_wait)</span><span class="params">(<span class="keyword">void</span>*)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*t_delete_event)</span><span class="params">(<span class="keyword">void</span>*)</span></span>;</span><br><span class="line">    t_opencl_wait p_opencl_wait;</span><br><span class="line">    t_imgdnn_wait p_imgdnn_wait;</span><br><span class="line">    t_delete_event p_delete;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> event_id = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 如果存在事件id,获取当前节点的事件id</span></span><br><span class="line">    <span class="keyword">if</span> (_global_event_id_.<span class="built_in">size</span>()) event_id = _global_event_id_[nid];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; depends.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="comment">// 从事件映射中获取设备事件列表</span></span><br><span class="line">        std::vector&lt;DeviceEvent&gt; &amp;elist = _global_nid_event_map_[depends[i]];</span><br><span class="line">        <span class="comment">// 获取设备事件列表的首个迭代器</span></span><br><span class="line">        std::vector&lt;DeviceEvent&gt;::iterator it = elist.<span class="built_in">begin</span>();</span><br><span class="line">        <span class="comment">// 遍历整个事件列表</span></span><br><span class="line">        <span class="keyword">while</span> (it != elist.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            DeviceEvent e = *it;</span><br><span class="line">            <span class="comment">// 如果事件为空或事件id与当前节点id不等，跳过</span></span><br><span class="line">            <span class="keyword">if</span> (e.event == <span class="literal">nullptr</span> || e.event_id != event_id) &#123;</span><br><span class="line">                it++;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Wait for event &quot;</span> &lt;&lt; (<span class="keyword">void</span>*) e.event</span><br><span class="line">                &lt;&lt; <span class="string">&quot; ID: &quot;</span> &lt;&lt; e.event_id &lt;&lt; <span class="string">&quot; is_opencl: &quot;</span> &lt;&lt; e.is_opencl;</span><br><span class="line">            <span class="comment">// 根据事件类型，调用不同的函数</span></span><br><span class="line">            <span class="keyword">if</span> (e.is_opencl) &#123;</span><br><span class="line">                p_opencl_wait = (t_opencl_wait) e.wait_for_event;</span><br><span class="line">                (*p_opencl_wait)(<span class="number">1</span>, &amp;e.event);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                p_imgdnn_wait = (t_imgdnn_wait) e.wait_for_event;</span><br><span class="line">                (*p_imgdnn_wait)(e.event);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 事件销毁</span></span><br><span class="line">            p_delete = (t_delete_event) e.destroy_event;</span><br><span class="line">            (*p_delete)(e.event);</span><br><span class="line">            e.event = <span class="literal">nullptr</span>;</span><br><span class="line">            <span class="comment">// 从事件列表中删除</span></span><br><span class="line">            it = elist.<span class="built_in">erase</span>(it);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Done with WaitForEvent for nid &quot;</span> &lt;&lt; nid</span><br><span class="line">        &lt;&lt; <span class="string">&quot; ID: &quot;</span> &lt;&lt; _global_event_id_[nid];</span><br><span class="line"></span><br><span class="line">    _global_event_id_[nid]++;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>MarkBuffers</strong>函数</li>
</ul>
<p>该函数将输出缓冲区和输入缓冲区标注为相应状态。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::MarkBuffers</span><span class="params">(<span class="keyword">uint32_t</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取图节点对象</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[index];</span><br><span class="line">    <span class="comment">/* Mark all output buffers as filled */</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> o = <span class="number">0</span>; o &lt; inode.param.num_outputs; ++o) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(index, o);</span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">        flags[eid] = PVR_BUFFER_FILLED; <span class="comment">/* In Use -&gt; Filled */</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Propagate inference id from input buffer to output.</span></span><br><span class="line"><span class="comment">     * Use inference id of a non-fixed input, as we don&#x27;t</span></span><br><span class="line"><span class="comment">     * (or can&#x27;t) propagate inference ids to fixed inputs.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">        <span class="keyword">int</span> input_infer_id = inference_number;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; ie : inode.inputs) &#123;</span><br><span class="line">            <span class="keyword">uint32_t</span> i_eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(ie.node_id, ie.index);</span><br><span class="line">            <span class="keyword">if</span> (is_fixed[i_eid]) <span class="keyword">continue</span>;</span><br><span class="line">            input_infer_id = infer_id[i_eid];</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        infer_id[eid] = input_infer_id;</span><br><span class="line">        <span class="comment">/* Network output nodes have -1 consumers */</span></span><br><span class="line">        <span class="keyword">if</span> (consumer_count[eid] == <span class="number">-1</span>) &#123;</span><br><span class="line">            flags[eid] = PVR_BUFFER_CONSUMED; <span class="comment">/* In Use -&gt; Consumed */</span></span><br><span class="line">            <span class="keyword">if</span> (is_fixed[eid]) flags[eid] = PVR_BUFFER_FILLED; <span class="comment">/* Params are always filled */</span></span><br><span class="line">            expected_outputs--;</span><br><span class="line">            <span class="built_in">DumpData</span>(eid, infer_id[eid], o);</span><br><span class="line">        &#125;</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; e : inode.inputs) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(e.node_id, e.index);</span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">        consumer_count[eid] -= <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (consumer_count[eid] == <span class="number">0</span>) &#123;</span><br><span class="line">            consumer_count[eid] = attrs_.consumers[eid];</span><br><span class="line">            flags[eid] = PVR_BUFFER_CONSUMED; <span class="comment">/* In Use -&gt; Consumed */</span></span><br><span class="line">            <span class="keyword">if</span> (is_fixed[eid]) flags[eid] = PVR_BUFFER_FILLED; <span class="comment">/* Params are always filled */</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">           	* If there are more than one consumers of it then</span></span><br><span class="line"><span class="comment">           	* mark this buffer as filled, otherwise we won&#x27;t be</span></span><br><span class="line"><span class="comment">           	* able to schedule consumers of it.</span></span><br><span class="line"><span class="comment">          	*/</span></span><br><span class="line">        	flags[eid] = PVR_BUFFER_FILLED;</span><br><span class="line">        &#125;</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="nna执行">2.2.1. NNA执行</h4>
<p>关于<code>NNA</code>设备的执行线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::ExecuteNNA</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(nna_mx)</span></span>;</span><br><span class="line">        <span class="comment">// 当收到另外一个线程的通知后，且条件为队列非空的情况下，线程阻塞解除；</span></span><br><span class="line">        nna_cv.<span class="built_in">wait</span>(lk, [<span class="keyword">this</span>]()&#123; <span class="keyword">return</span> !<span class="keyword">this</span>-&gt;nna_queue.<span class="built_in">empty</span>(); &#125;);</span><br><span class="line">        <span class="comment">// 获取队列的首个索引</span></span><br><span class="line">        <span class="keyword">uint32_t</span> index = nna_queue.<span class="built_in">front</span>();</span><br><span class="line">        nna_queue.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="comment">// 如果索引号为-1，线程终止</span></span><br><span class="line">        <span class="keyword">if</span> ((<span class="keyword">int</span>)index == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="built_in">UnlockDevice</span>(kDLPowerVRNNA);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            <span class="keyword">break</span>;<span class="comment">// 终止线程</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Wait for buffers to fill if it is a device copy */</span></span><br><span class="line">        <span class="comment">// 根据索引号，获取节点对象</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[index];</span><br><span class="line">        <span class="comment">// 获取节点的参数</span></span><br><span class="line">        <span class="keyword">const</span> TVMOpParam&amp; param = inode.param;</span><br><span class="line">        <span class="comment">// 如果函数名为&quot;__copy&quot;，则该操作节点负责数据的跨设备拷贝</span></span><br><span class="line">        <span class="keyword">if</span> (param.func_name == <span class="string">&quot;__copy&quot;</span>) &#123;</span><br><span class="line">            <span class="built_in">WaitForEvent</span>(index);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Execute it */</span></span><br><span class="line">        op_execs_[index]();</span><br><span class="line">        <span class="built_in">UnlockDevice</span>(kDLPowerVRNNA);</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">        <span class="comment">//标注缓存状态</span></span><br><span class="line">        <span class="built_in">MarkBuffers</span>(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="nnpu执行">2.2.2. NNPU执行</h4>
<p>关于<code>NNPU</code>设备的执行线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::ExecuteNNPU</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(nnpu_mx)</span></span>;</span><br><span class="line">        nnpu_cv.<span class="built_in">wait</span>(lk, [<span class="keyword">this</span>]()&#123; <span class="keyword">return</span> !<span class="keyword">this</span>-&gt;nnpu_queue.<span class="built_in">empty</span>(); &#125;);</span><br><span class="line">        <span class="keyword">uint32_t</span> index = nnpu_queue.<span class="built_in">front</span>();</span><br><span class="line">        nnpu_queue.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span> ((<span class="keyword">int</span>)index == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="built_in">UnlockDevice</span>(kDLPowerVRNNPU);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Wait for buffers to fill if it is a device copy */</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[index];</span><br><span class="line">        <span class="keyword">const</span> TVMOpParam&amp; param = inode.param;</span><br><span class="line">        <span class="keyword">if</span> (param.func_name == <span class="string">&quot;__copy&quot;</span>) &#123;</span><br><span class="line">            <span class="built_in">WaitForEvent</span>(index);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Execute it */</span></span><br><span class="line">        op_execs_[index]();</span><br><span class="line">        <span class="built_in">UnlockDevice</span>(kDLPowerVRNNPU);</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">        <span class="built_in">MarkBuffers</span>(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="cpu执行">2.2.3. CPU执行</h4>
<p>关于<code>CPU</code>设备的执行线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::ExecuteCPU</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(cpu_mx)</span></span>;</span><br><span class="line">        cpu_cv.<span class="built_in">wait</span>(lk, [<span class="keyword">this</span>]()&#123; <span class="keyword">return</span> !<span class="keyword">this</span>-&gt;cpu_queue.<span class="built_in">empty</span>(); &#125;);</span><br><span class="line">        <span class="keyword">uint32_t</span> index = cpu_queue.<span class="built_in">front</span>();</span><br><span class="line">        cpu_queue.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span> ((<span class="keyword">int</span>)index == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="built_in">UnlockDevice</span>(kDLCPU);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Wait for buffers to fill */</span></span><br><span class="line">        <span class="built_in">WaitForEvent</span>(index);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* Execute it */</span></span><br><span class="line">        op_execs_[index]();</span><br><span class="line">        <span class="built_in">UnlockDevice</span>(kDLCPU);</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">        <span class="built_in">MarkBuffers</span>(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="流水线执行">2.3.流水线执行</h3>
<p>关于流水线运行，主要分为如下几步：获取输入、更新准备列表、调度准备列表和执行完毕后的后处理。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> stop_exec = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">bool</span> nothing_to_fetch = <span class="literal">false</span>;</span><br><span class="line">std::vector&lt;<span class="keyword">uint32_t</span>&gt; ready_list;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Pipelined execution */</span></span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    <span class="comment">/* Fetch inputs */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Prepare ready list */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Schedule ready list */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Excution Finish post-processing */</span></span><br><span class="line">    <span class="keyword">if</span> (nothing_to_fetch == <span class="literal">true</span>  &amp;&amp; </span><br><span class="line">        ready_list.<span class="built_in">size</span>() == <span class="number">0</span>  &amp;&amp; </span><br><span class="line">        has_scheduled == <span class="literal">false</span>   &amp;&amp; </span><br><span class="line">        done_infers == <span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//post-processing</span></span><br><span class="line"></span><br><span class="line">        stop_exec = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">while</span> (stop_exec == <span class="literal">false</span>);</span><br></pre></td></tr></table></figure>
<h4 id="获取输入">2.3.1.获取输入</h4>
<ul>
<li><strong>inputs_waiting</strong> 说明</li>
</ul>
<p>函数<code>int GraphRuntime::FetchInputs()</code>从<code>inputs_waiting</code>中获取输入集。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::unordered_map&lt;<span class="keyword">int</span>, std::vector&lt;DLTensor*&gt;&gt; inputs_waiting;</span><br></pre></td></tr></table></figure>
<p><code>DLTensor</code>用于表示C格式的张量结构体</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment">* \brief Plain C Tensor object, does not manage memory.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="comment">/*!</span></span><br><span class="line"><span class="comment">    * \brief The opaque data pointer points to the allocated data.</span></span><br><span class="line"><span class="comment">    *  This will be CUDA device pointer or cl_mem handle in OpenCL.</span></span><br><span class="line"><span class="comment">    *  This pointer is always aligns to 256 bytes as in CUDA.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">void</span>* data;</span><br><span class="line">    <span class="comment">/*! \brief The device context of the tensor */</span></span><br><span class="line">    DLContext ctx;</span><br><span class="line">    <span class="comment">/*! \brief Number of dimensions */</span></span><br><span class="line">    <span class="keyword">int</span> ndim;</span><br><span class="line">    <span class="comment">/*! \brief The data type of the pointer*/</span></span><br><span class="line">    DLDataType dtype;</span><br><span class="line">    <span class="comment">/*! \brief The shape of the tensor */</span></span><br><span class="line">    <span class="keyword">int64_t</span>* shape;</span><br><span class="line">    <span class="comment">/*!</span></span><br><span class="line"><span class="comment">    * \brief strides of the tensor,</span></span><br><span class="line"><span class="comment">    *  can be NULL, indicating tensor is compact.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">int64_t</span>* strides;</span><br><span class="line">    <span class="comment">/*! \brief The offset in bytes to the beginning pointer to data */</span></span><br><span class="line">    <span class="keyword">uint64_t</span> byte_offset;</span><br><span class="line">&#125; DLTensor;</span><br></pre></td></tr></table></figure>
<ul>
<li>捕获输入状态如下</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Fetch input return values */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FETCH_CANNOT_FILL       1 <span class="comment">// 输入未满</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FETCH_INPUT_FILLED      2 <span class="comment">// 输入满</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FETCH_NO_INPUTS         3 <span class="comment">// 无输入数据</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>缓存标志</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Buffer flags */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PVR_BUFFER_EMPTY        0 <span class="comment">//缓存区 空</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PVR_BUFFER_FILLED       1 <span class="comment">//缓存区 满</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PVR_BUFFER_INUSE        2 <span class="comment">//缓存区 使用中</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PVR_BUFFER_CONSUMED     3 <span class="comment">//缓存区 已消费</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>FetchInputs()</strong>函数</li>
</ul>
<p>判断输入缓冲区的数据是否有效.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GraphRuntime::FetchInputs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始返回值&quot;未填充&quot;</span></span><br><span class="line">    <span class="keyword">int</span> ret = FETCH_CANNOT_FILL;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果没有更多输入数据处理，直接返回&quot;无输入数据状态&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (inputs_waiting.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        <span class="keyword">return</span> FETCH_NO_INPUTS;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历输入集合</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;i : inputs_waiting) &#123;</span><br><span class="line">        <span class="comment">// 获取输入索引号</span></span><br><span class="line">        <span class="keyword">int</span> index = i.first;</span><br><span class="line">        <span class="comment">// 获取输入张量值</span></span><br><span class="line">        std::vector&lt;DLTensor*&gt; &amp;in_list = i.second;</span><br><span class="line">        <span class="comment">// 如果张量大小不为0</span></span><br><span class="line">        <span class="keyword">if</span> (in_list.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            <span class="comment">//根据输入索引号，获取条目节点的索引</span></span><br><span class="line">            <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(input_nodes_[index], <span class="number">0</span>);</span><br><span class="line">            <span class="comment">//创建线程锁</span></span><br><span class="line">            <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">            <span class="comment">// 如果缓存空或已消费</span></span><br><span class="line">            <span class="keyword">if</span> (flags[eid] == PVR_BUFFER_EMPTY ||</span><br><span class="line">                flags[eid] == PVR_BUFFER_CONSUMED) &#123;</span><br><span class="line">                <span class="comment">// 将inputs_waiting中的张量数据拷贝到数据输入节点</span></span><br><span class="line">                data_entry_[eid].<span class="built_in">CopyFrom</span>(in_list[<span class="number">0</span>]);</span><br><span class="line">                <span class="comment">// 擦除inputs_waiting中的张量数据</span></span><br><span class="line">                in_list.<span class="built_in">erase</span>(in_list.<span class="built_in">begin</span>());</span><br><span class="line">                <span class="comment">// 从附加图属性中获取消费者</span></span><br><span class="line">                consumer_count[eid] = attrs_.consumers[eid];</span><br><span class="line">                <span class="comment">// 缓存标志设置为满</span></span><br><span class="line">                flags[eid] = PVR_BUFFER_FILLED;    <span class="comment">/* Empty or Consumed -&gt; Filled */</span></span><br><span class="line">                <span class="comment">// 设置节点为非固定输入类型</span></span><br><span class="line">                is_fixed[eid] = <span class="number">0</span>;</span><br><span class="line">                <span class="comment">// 保存当前推理计数</span></span><br><span class="line">                infer_id[eid] = inference_number;</span><br><span class="line">                ret = FETCH_INPUT_FILLED;</span><br><span class="line">            &#125;</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/*</span></span><br><span class="line"><span class="comment">     * If all inputs of this network are filled.</span></span><br><span class="line"><span class="comment">     * Set expected number of outputs this inference should</span></span><br><span class="line"><span class="comment">     * produce. If operation of this node is null then the</span></span><br><span class="line"><span class="comment">     * node is not executed. This output buffer is already</span></span><br><span class="line"><span class="comment">     * filled by parameters. See Yolov2 network.</span></span><br><span class="line"><span class="comment">  	 */</span></span><br><span class="line">    <span class="comment">// 如果存在输入捕获</span></span><br><span class="line">    <span class="keyword">if</span> (ret == FETCH_INPUT_FILLED) &#123;</span><br><span class="line">        <span class="keyword">bool</span> all_inputs_filled = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i_nid = <span class="number">0</span>; i_nid &lt; input_nodes_.<span class="built_in">size</span>(); i_nid++) &#123;</span><br><span class="line">            <span class="keyword">uint32_t</span> i_idx = input_nodes_[i_nid];</span><br><span class="line">            <span class="keyword">uint32_t</span> i_eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(i_idx, <span class="number">0</span>);</span><br><span class="line">            <span class="comment">// 如果节点为&quot;null&quot;跳过执行</span></span><br><span class="line">            <span class="keyword">if</span> (op_execs_[i_idx] == <span class="literal">nullptr</span>) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">// 如果输入未满，置标志位all_inputs_filled为false</span></span><br><span class="line">            <span class="keyword">if</span> (flags[i_eid] != PVR_BUFFER_FILLED) &#123;</span><br><span class="line">                all_inputs_filled = <span class="literal">false</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果所有输入都满，设置该推理期望的输出数量</span></span><br><span class="line">        <span class="keyword">if</span> (all_inputs_filled == <span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">uint32_t</span> o_nid = <span class="number">0</span>; o_nid &lt; outputs_.<span class="built_in">size</span>(); o_nid++) &#123;</span><br><span class="line">                <span class="keyword">auto</span> e = outputs_[o_nid];</span><br><span class="line">                <span class="keyword">uint32_t</span> o_eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(e);</span><br><span class="line">                <span class="comment">// 如果操作为空</span></span><br><span class="line">                <span class="keyword">if</span> (op_execs_[e.node_id] == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                    <span class="built_in">DumpData</span>(o_eid, inference_number, o_nid);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    std::unique_lock&lt;std::mutex&gt; <span class="built_in">lk</span>(data_mx);</span><br><span class="line">                    expected_outputs++;</span><br><span class="line">                    lk.<span class="built_in">unlock</span>();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 推理计数递增1</span></span><br><span class="line">            <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">            inference_number++;     <span class="comment">/* start a new inference */</span></span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/* Erase empty entries from map */</span></span><br><span class="line">    std::unordered_map&lt;<span class="keyword">int</span>, std::vector&lt;DLTensor*&gt;&gt;::iterator it </span><br><span class="line">        = inputs_waiting.<span class="built_in">begin</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (it != inputs_waiting.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (it-&gt;second.<span class="built_in">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            it = inputs_waiting.<span class="built_in">erase</span>(it);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            it++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="更新准备列表">2.3.2. 更新准备列表</h4>
<p>通过判断节点的输入缓冲区是否满和输出缓冲区是否被使用等状态，决定节点是否推入准备列表。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从图节点获取节点数量</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">uint32_t</span> nid = <span class="number">0</span>; nid &lt; <span class="keyword">this</span>-&gt;<span class="built_in">GetNumOfNodes</span>(); ++nid) &#123;</span><br><span class="line">    <span class="comment">// 根据节点id获取图节点对象</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[nid];</span><br><span class="line">    <span class="comment">// 根据节点id，获取节点的入口索引</span></span><br><span class="line">    <span class="keyword">int</span> node_eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(nid, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 如果节点类型为null，说明该节点是参数而非算子操作，故跳过处理</span></span><br><span class="line">    <span class="keyword">if</span> (inode.op_type == <span class="string">&quot;null&quot;</span>) <span class="keyword">continue</span>;</span><br><span class="line">    <span class="comment">// 查找当前节点是否在ready_list中</span></span><br><span class="line">    std::vector&lt;<span class="keyword">uint32_t</span>&gt;::iterator it = std::<span class="built_in">find</span>(ready_list.<span class="built_in">begin</span>(), ready_list.<span class="built_in">end</span>(), nid);</span><br><span class="line">    <span class="comment">// 如果当前节点的id已经在ready_list中，则跳过当前节点的处理；</span></span><br><span class="line">    <span class="keyword">if</span> (it != ready_list.<span class="built_in">end</span>()) <span class="keyword">continue</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">bool</span> inputs_ready = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">bool</span> outputs_consumed = <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 遍历图节点的所有输入</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; e : inode.inputs) &#123;</span><br><span class="line">        <span class="comment">// 根据节点的输入参数node_id和index,获取节点入口索引</span></span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(e.node_id, e.index);</span><br><span class="line">        <span class="comment">// 根据入口节点获取缓存状态</span></span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">        <span class="keyword">int</span> flag = flags[eid];</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * If there are more than one consumers of same buffer</span></span><br><span class="line"><span class="comment">         * then we might re-schedule old node again just by looking </span></span><br><span class="line"><span class="comment">         * at the FILLED flag. If infer ID of node is same as input</span></span><br><span class="line"><span class="comment">         * buffer then we have already used this buffer for node_eid.</span></span><br><span class="line"><span class="comment">         * Wait for another set of inputs to be loaded and then </span></span><br><span class="line"><span class="comment">         * re-schedule the node again. Note that we do not propagate</span></span><br><span class="line"><span class="comment">         * infer IDs to fixed inputs.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 1. 如果同一缓冲区有多个使用者，那么可以仅通过查看FILLED标志重新安排旧节点；</span></span><br><span class="line"><span class="comment">         * 2. 如果当前节点和输入缓冲区的推理ID相同，则当前节点的缓冲区已经被使用，等待另一个输入集被加载，然后再次调度该节点；</span></span><br><span class="line"><span class="comment">         * 3. 请注意，我们不传布推理id为固定输入；</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       	<span class="comment">// 当为非固定输入且 当前节点与输入缓冲区的推理节点相等时，说明缓冲区已经被使用</span></span><br><span class="line">        <span class="keyword">if</span> (is_fixed[eid] == <span class="number">0</span> &amp;&amp; infer_id[node_eid] == infer_id[eid]) &#123;</span><br><span class="line">            inputs_ready = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (flag != PVR_BUFFER_FILLED) &#123;</span><br><span class="line">            inputs_ready = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            inputs_ready = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 输入未准备好，退出</span></span><br><span class="line">    <span class="keyword">if</span> (inputs_ready == <span class="literal">false</span>) <span class="keyword">continue</span>;</span><br><span class="line">    <span class="comment">// 遍历输出</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> o = <span class="number">0</span>; o &lt; inode.param.num_outputs; ++o) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(nid, o);</span><br><span class="line">        <span class="comment">// 获取缓存区状态</span></span><br><span class="line">        <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">        <span class="keyword">int</span> flag = flags[eid];</span><br><span class="line">        lk.<span class="built_in">unlock</span>();</span><br><span class="line">        <span class="comment">// 如果输出缓存区慢或者在使用中，则outputs_consumed为false,表明输出缓冲区未被使用</span></span><br><span class="line">        <span class="keyword">if</span> (flag == PVR_BUFFER_FILLED || flag == PVR_BUFFER_INUSE) &#123;</span><br><span class="line">            outputs_consumed = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            outputs_consumed = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果输出缓冲区未被使用，则退出</span></span><br><span class="line">    <span class="keyword">if</span> (outputs_consumed == <span class="literal">false</span>) <span class="keyword">continue</span>;</span><br><span class="line">    <span class="comment">/* Mark all input and output buffers as in use */</span></span><br><span class="line">    <span class="comment">// 将所有输入和输出缓冲区标记为使用中</span></span><br><span class="line">    <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; e : inode.inputs) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(e.node_id, e.index);</span><br><span class="line">        flags[eid] = PVR_BUFFER_INUSE;    <span class="comment">/* Filled -&gt; In Use */</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> o = <span class="number">0</span>; o &lt; inode.param.num_outputs; ++o) &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(nid, o);</span><br><span class="line">        flags[eid] = PVR_BUFFER_INUSE;    <span class="comment">/* Empty or Consumed -&gt; In Use */</span></span><br><span class="line">    &#125;</span><br><span class="line">    lk.<span class="built_in">unlock</span>();</span><br><span class="line">    <span class="comment">// 将当前节点推送到准备好列表</span></span><br><span class="line">    ready_list.<span class="built_in">push_back</span>(nid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="调度准备列表">2.3.3. 调度准备列表</h4>
<p>下述代码主要根据上一步骤推入的有效节点，依次将这些节点推送到相应设备队列中，等待设备执行。设备类型主要为<code>NNA</code>，<code>NNPU</code>和<code>CPU</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调度状态初始化为false</span></span><br><span class="line"><span class="keyword">bool</span> has_scheduled = <span class="literal">false</span>;</span><br><span class="line"><span class="comment">// 获取准备列表的首个迭代器</span></span><br><span class="line">std::vector&lt;<span class="keyword">uint32_t</span>&gt;::iterator it = ready_list.<span class="built_in">begin</span>();</span><br><span class="line"><span class="comment">// 遍历整个准备列表</span></span><br><span class="line"><span class="keyword">while</span> (it != ready_list.<span class="built_in">end</span>()) &#123;</span><br><span class="line">    <span class="comment">// 获取节点索引</span></span><br><span class="line">    <span class="keyword">int</span> index = *it;</span><br><span class="line">    <span class="comment">// 获取输入入口节点索引</span></span><br><span class="line">    <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(*it, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 通过入口索引号获取设备索引</span></span><br><span class="line">    <span class="keyword">int</span> device = attrs_.device_index[eid];</span><br><span class="line">    <span class="comment">// 判断该设备是否有效，即设备是否锁住正在运行</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">IsDeviceAvailable</span>(device)) &#123;</span><br><span class="line">        <span class="comment">// 从准备列表中删除当前节点</span></span><br><span class="line">        it = ready_list.<span class="built_in">erase</span>(it);</span><br><span class="line">        <span class="comment">// 如果NNA 设备</span></span><br><span class="line">        <span class="keyword">if</span> (device == kDLPowerVRNNA) &#123;</span><br><span class="line">            <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(nna_mx)</span></span>;</span><br><span class="line">            <span class="comment">// 锁定NNA设备</span></span><br><span class="line">            <span class="built_in">LockDevice</span>(kDLPowerVRNNA);</span><br><span class="line">            <span class="comment">// 将节点的索引推入nna队列</span></span><br><span class="line">            nna_queue.<span class="built_in">push</span>(index);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            <span class="comment">// 唤醒nna等待线程</span></span><br><span class="line">            nna_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">            <span class="comment">// 设置调度状态为真</span></span><br><span class="line">            has_scheduled = <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( device == kDLPowerVRNNPU ||</span><br><span class="line">                   device == kDLOpenCL ||</span><br><span class="line">                   device == kDLPowerVROpenCL) &#123;</span><br><span class="line">            std::unique_lock&lt;std::mutex&gt; <span class="built_in">lk</span>(nnpu_mx);</span><br><span class="line">            <span class="built_in">LockDevice</span>(kDLPowerVRNNPU);</span><br><span class="line">            nnpu_queue.<span class="built_in">push</span>(index);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            nnpu_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">            has_scheduled = <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( device == kDLCPU ||</span><br><span class="line">                   device == kDLPowerVRPython ||</span><br><span class="line">                   device == kDLPowerVRCPP) &#123;</span><br><span class="line">            std::unique_lock&lt;std::mutex&gt; <span class="built_in">lk</span>(cpu_mx);</span><br><span class="line">            <span class="built_in">LockDevice</span>(kDLCPU);</span><br><span class="line">            cpu_queue.<span class="built_in">push</span>(index);</span><br><span class="line">            lk.<span class="built_in">unlock</span>();</span><br><span class="line">            cpu_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">            has_scheduled = <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果设备类型非上述设备，则迭代器递增</span></span><br><span class="line">            it++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="后处理">2.3.4. 后处理</h4>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk</span><span class="params">(data_mx)</span></span>;</span><br><span class="line"><span class="keyword">bool</span> done_infers = (expected_outputs == <span class="number">0</span>);</span><br><span class="line">lk.<span class="built_in">unlock</span>();</span><br><span class="line"><span class="comment">// 1. 未捕获到输入 </span></span><br><span class="line"><span class="comment">// 2. 准备列表为空</span></span><br><span class="line"><span class="comment">// 3. 没有调度的设备</span></span><br><span class="line"><span class="comment">// 4. 完成推理流程</span></span><br><span class="line"><span class="keyword">if</span> (nothing_to_fetch == <span class="literal">true</span> &amp;&amp; ready_list.<span class="built_in">size</span>() == <span class="number">0</span> &amp;&amp;</span><br><span class="line">    has_scheduled == <span class="literal">false</span> &amp;&amp; done_infers == <span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 往相应的设备队列中推送-1，并唤醒相应线程，使得线程终止</span></span><br><span class="line">    <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk_nna</span><span class="params">(nna_mx)</span></span>;</span><br><span class="line">    <span class="built_in">LockDevice</span>(kDLPowerVRNNA);</span><br><span class="line">    nna_queue.<span class="built_in">push</span>(<span class="number">-1</span>);</span><br><span class="line">    nna_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">    lk_nna.<span class="built_in">unlock</span>();</span><br><span class="line">    <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk_nnpu</span><span class="params">(nnpu_mx)</span></span>;</span><br><span class="line">    <span class="built_in">LockDevice</span>(kDLPowerVRNNPU);</span><br><span class="line">    nnpu_queue.<span class="built_in">push</span>(<span class="number">-1</span>);</span><br><span class="line">    nnpu_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">    lk_nnpu.<span class="built_in">unlock</span>();</span><br><span class="line">    <span class="function">std::unique_lock&lt;std::mutex&gt; <span class="title">lk_cpu</span><span class="params">(cpu_mx)</span></span>;</span><br><span class="line">    <span class="built_in">LockDevice</span>(kDLCPU);</span><br><span class="line">    cpu_queue.<span class="built_in">push</span>(<span class="number">-1</span>);</span><br><span class="line">    cpu_cv.<span class="built_in">notify_one</span>();</span><br><span class="line">    lk_cpu.<span class="built_in">unlock</span>();</span><br><span class="line">    <span class="comment">// 设置停止执行状态为真</span></span><br><span class="line">    stop_exec = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果相应设备状态为真，则等待响应设备线程终止；</span></span><br><span class="line"><span class="keyword">if</span> (need_nna == <span class="literal">true</span>) nna_th.<span class="built_in">join</span>();</span><br><span class="line"><span class="keyword">if</span> (need_nnpu == <span class="literal">true</span>) nnpu_th.<span class="built_in">join</span>();</span><br><span class="line"><span class="keyword">if</span> (need_cpu == <span class="literal">true</span>) cpu_th.<span class="built_in">join</span>();</span><br></pre></td></tr></table></figure>
<h2 id="init函数">3. Init函数</h2>
<p>该函数在创建运行环境时调用该初始化函数，通过传入模型图和</p>
<figure>
<img src="C:\Users\Zhu%20Guohua\Pictures\powervr_tvm.png" alt="powervr_tvm" /><figcaption aria-hidden="true">powervr_tvm</figcaption>
</figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::Init</span><span class="params">(<span class="keyword">const</span> std::string&amp; graph_json,</span></span></span><br><span class="line"><span class="params"><span class="function">                        tvm::runtime::Module <span class="keyword">module</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> std::vector&lt;TVMContext&gt;&amp; ctxs)</span> </span>&#123;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">ifndef</span> _LIBCPP_SGX_NO_IOSTREAMS</span></span><br><span class="line">    <span class="function">std::istringstream <span class="title">is</span><span class="params">(graph_json)</span></span>;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    std::string is = graph_json;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="function">dmlc::JSONReader <span class="title">reader</span><span class="params">(&amp;is)</span></span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">Load</span>(&amp;reader);</span><br><span class="line">    module_ = <span class="keyword">module</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Disable wrapper or the creation of single context if</span></span><br><span class="line"><span class="comment">   * user has set DISABLE_WRAPPER to 1.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">    <span class="keyword">char</span> *env_disable_wrapper = <span class="built_in">getenv</span>(<span class="string">&quot;DISABLE_WRAPPER&quot;</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;disable_wrapper = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (env_disable_wrapper &amp;&amp; env_disable_wrapper[<span class="number">0</span>] == <span class="string">&#x27;1&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;disable_wrapper = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Check if we can do pipelined execution */</span></span><br><span class="line">    <span class="keyword">if</span> (attrs_.device_index.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        enable_pipe_exe = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!attrs_.device_ctxs.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        TVMContext ctx;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;disable_wrapper == <span class="literal">false</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span>&amp; dev : attrs_.device_ctxs) &#123;</span><br><span class="line">                <span class="comment">/* Use single context for NNA and NNPU to enable zero copy */</span></span><br><span class="line">                <span class="keyword">if</span> (dev == kDLPowerVRNNA || dev == kDLPowerVRNNPU) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">/* Create other contexts like Python, CPP or OpenCL */</span></span><br><span class="line">                ctx.device_type = <span class="keyword">static_cast</span>&lt;DLDeviceType&gt;(dev);</span><br><span class="line">                ctx.device_id = <span class="number">0</span>;</span><br><span class="line">                ctxs_.<span class="built_in">push_back</span>(ctx);</span><br><span class="line">            &#125;</span><br><span class="line">            ctx.device_type = <span class="keyword">static_cast</span>&lt;DLDeviceType&gt;(kDLPowerVR);</span><br><span class="line">            ctx.device_id = <span class="number">0</span>;</span><br><span class="line">            ctxs_.<span class="built_in">push_back</span>(ctx);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span>&amp; dev : attrs_.device_ctxs) &#123;</span><br><span class="line">                ctx.device_type = <span class="keyword">static_cast</span>&lt;DLDeviceType&gt;(dev);</span><br><span class="line">                ctx.device_id = <span class="number">0</span>;</span><br><span class="line">                ctxs_.<span class="built_in">push_back</span>(ctx);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ctx.device_type = kDLCPU;</span><br><span class="line">        ctx.device_id = <span class="number">0</span>;</span><br><span class="line">        ctxs_.<span class="built_in">push_back</span>(ctx);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ctxs_ = ctxs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">SetupStorage</span>();</span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">SetupOpExecs</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="配置存储">3.1. 配置存储</h3>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::SetupStorage</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Grab saved optimization plan from graph.</span></span><br><span class="line">    <span class="comment">// 从图中获取数据类型</span></span><br><span class="line">    std::vector&lt;TVMType&gt; vtype;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> std::string&amp; s_type : attrs_.dltype) &#123;</span><br><span class="line">        vtype.<span class="built_in">push_back</span>(tvm::runtime::<span class="built_in">String2TVMType</span>(s_type));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Size and device type of each storage pool entry.</span></span><br><span class="line">    std::vector&lt;PoolEntry&gt; pool_entry;</span><br><span class="line">    <span class="comment">// Find the maximum space size.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; attrs_.shape.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        <span class="keyword">int</span> storage_id = attrs_.storage_id[i];</span><br><span class="line">        <span class="comment">// Use the fallback device if no device index is available.</span></span><br><span class="line">        <span class="keyword">int</span> device_type = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(ctxs_[<span class="number">0</span>].device_type);</span><br><span class="line">        <span class="keyword">if</span> (!attrs_.device_index.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            device_type = attrs_.device_index[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果未禁用包函数，设备类型未NNA或NNPU的设备都设置为PowerVR</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;disable_wrapper == <span class="literal">false</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (device_type == kDLPowerVRNNA || device_type == kDLPowerVRNNPU) &#123;</span><br><span class="line">                device_type = kDLPowerVR;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算数据大小</span></span><br><span class="line">        <span class="keyword">size_t</span> size = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int64_t</span> sz : attrs_.shape[i]) &#123;</span><br><span class="line">            size *= <span class="keyword">static_cast</span>&lt;<span class="keyword">size_t</span>&gt;(sz);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">CHECK_GE</span>(storage_id, <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Do not support runtime shape op&quot;</span>;</span><br><span class="line">        <span class="comment">// 计算占用得字节数</span></span><br><span class="line">        DLDataType t = vtype[i];</span><br><span class="line">        <span class="keyword">size_t</span> bits = t.bits * t.lanes;</span><br><span class="line">        <span class="built_in">CHECK</span>(bits % <span class="number">8U</span> ==  <span class="number">0U</span> || bits ==<span class="number">1U</span>);</span><br><span class="line">        <span class="keyword">size_t</span> bytes = ((bits + <span class="number">7U</span>) / <span class="number">8U</span>) * size;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 内存池入口配置，扩展大小，存储最大字节数和设备类型</span></span><br><span class="line">        <span class="keyword">uint32_t</span> sid = <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(storage_id);</span><br><span class="line">        <span class="keyword">if</span> (sid &gt;= pool_entry.<span class="built_in">size</span>()) &#123;</span><br><span class="line">            pool_entry.<span class="built_in">resize</span>(sid + <span class="number">1</span>, &#123;<span class="number">0</span>, <span class="number">-1</span>&#125;);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">CHECK</span>(pool_entry[sid].device_type == <span class="number">-1</span> ||</span><br><span class="line">                  pool_entry[sid].device_type == device_type)</span><br><span class="line">                &lt;&lt; <span class="string">&quot;The same pool entry cannot be assigned to multiple devices&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pool_entry[sid].size = std::<span class="built_in">max</span>(pool_entry[sid].size, bytes);</span><br><span class="line">        pool_entry[sid].device_type = device_type;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate the space.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; pit : pool_entry) &#123;</span><br><span class="line">        std::vector&lt;<span class="keyword">int64_t</span>&gt; shape;</span><br><span class="line">        <span class="comment">// This for loop is very fast since there are usually only a couple of</span></span><br><span class="line">        <span class="comment">// devices available on the same hardware.</span></span><br><span class="line">        <span class="comment">// ctxs主要包含device_type和device_id</span></span><br><span class="line">        <span class="comment">// 下述代码遍历整个ctxs，是否存在上下文参数的设备类型与内存池的设备类型相同，则返回该上下文参数的迭代</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; cit =</span><br><span class="line">            std::<span class="built_in">find_if</span>(ctxs_.<span class="built_in">begin</span>(), ctxs_.<span class="built_in">end</span>(), [&amp;pit](<span class="keyword">const</span> TVMContext&amp; c) &#123;</span><br><span class="line">                <span class="keyword">return</span> pit.device_type == <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(c.device_type);</span><br><span class="line">            &#125;);</span><br><span class="line">        <span class="comment">// 如果不匹配，则返回首个上下文参数</span></span><br><span class="line">        TVMContext ctx = cit == ctxs_.<span class="built_in">end</span>() ? ctxs_[<span class="number">0</span>] : *cit;</span><br><span class="line">        <span class="comment">// 按照4字节推入数据池的大小</span></span><br><span class="line">        shape.<span class="built_in">push_back</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">int64_t</span>&gt;(pit.size + <span class="number">3</span>) / <span class="number">4</span>);</span><br><span class="line">        storage_pool_.<span class="built_in">push_back</span>(</span><br><span class="line">            NDArray::<span class="built_in">Empty</span>(shape, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, ctx));<span class="comment">// 根据shape和ctx,申请内存空间</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果流水线执行使能，根据节点得入口数，配置消费计数、缓存状态、推理id和固定输入状态数组的大小</span></span><br><span class="line">    <span class="keyword">if</span> (enable_pipe_exe) &#123;</span><br><span class="line">        consumer_count.<span class="built_in">resize</span>(<span class="built_in">num_node_entries</span>());</span><br><span class="line">        flags.<span class="built_in">resize</span>(<span class="built_in">num_node_entries</span>());</span><br><span class="line">        infer_id.<span class="built_in">resize</span>(<span class="built_in">num_node_entries</span>());</span><br><span class="line">        is_fixed.<span class="built_in">resize</span>(<span class="built_in">num_node_entries</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Assign the pooled entries. A unified memory pool is used to simplifiy</span></span><br><span class="line">    <span class="comment">// memory assignment for each node entry. The allocated memory on each device</span></span><br><span class="line">    <span class="comment">// is mapped to this pool.</span></span><br><span class="line">    data_entry_.<span class="built_in">resize</span>(<span class="built_in">num_node_entries</span>());<span class="comment">//node_row_ptr_数组的最后一个元素配置数据入口的数组大小</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; data_entry_.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">        <span class="comment">// 索引0默认存储输出缓存区地址，</span></span><br><span class="line">        <span class="keyword">int</span> storage_id = attrs_.storage_id[i];</span><br><span class="line">        <span class="built_in">CHECK_LT</span>(<span class="keyword">static_cast</span>&lt;<span class="keyword">size_t</span>&gt;(storage_id), storage_pool_.<span class="built_in">size</span>());</span><br><span class="line">        <span class="comment">// 更新数据的入口地址</span></span><br><span class="line">        data_entry_[i] =</span><br><span class="line">            storage_pool_[storage_id].<span class="built_in">CreateView</span>(attrs_.shape[i], vtype[i]);</span><br><span class="line">        <span class="comment">// 更新状态量</span></span><br><span class="line">        <span class="keyword">if</span> (enable_pipe_exe) &#123;</span><br><span class="line">            <span class="comment">// 消费初始值为1，最后输出为-1</span></span><br><span class="line">            consumer_count[i] = attrs_.consumers[i];</span><br><span class="line">            <span class="comment">// 缓存区使用状态都设置为空</span></span><br><span class="line">            flags[i] = PVR_BUFFER_EMPTY;   <span class="comment">/* Empty */</span></span><br><span class="line">            <span class="comment">// 推理计数-1</span></span><br><span class="line">            infer_id[i] = <span class="number">-1</span>;</span><br><span class="line">            <span class="comment">// 初始为非固定输入</span></span><br><span class="line">            is_fixed[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="配置执行操作">3.2. 配置执行操作</h3>
<p>如下图所示，节点配置时，上一个节点的输出缓存区是下一个节点的输入缓冲区，避免的重复的数据搬运。如果两个节点运行在不同设备，则自动插入一个拷贝函数，直接将上一个节点的输出缓冲区内容直接拷贝到下一个设备的输入缓冲区。</p>
<figure>
<img src="D:\WorkSpace\github\HexoBlog\source\draft\doc\powervr_tvm-memory.png" alt="powervr_tvm-memory" /><figcaption aria-hidden="true">powervr_tvm-memory</figcaption>
</figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GraphRuntime::SetupOpExecs</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 根据实际图的节点数初始化op_execs数组的大小</span></span><br><span class="line">    op_execs_.<span class="built_in">resize</span>(<span class="keyword">this</span>-&gt;<span class="built_in">GetNumOfNodes</span>());</span><br><span class="line">    <span class="comment">// setup the array and requirements.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> nid = <span class="number">0</span>; nid &lt; <span class="keyword">this</span>-&gt;<span class="built_in">GetNumOfNodes</span>(); ++nid) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; inode = nodes_[nid];</span><br><span class="line">        <span class="keyword">if</span> (inode.op_type == <span class="string">&quot;null&quot;</span>) <span class="keyword">continue</span>;<span class="comment">// 非操作节点跳过</span></span><br><span class="line">        _global_func_nid_map_[nid] = inode.param.func_name;<span class="comment">//将函数名更新到_global_func_nid_map_</span></span><br><span class="line">        <span class="comment">// 关于节点的输入和输出缓存地址</span></span><br><span class="line">        std::vector&lt;DLTensor&gt; args;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; e : inode.inputs) &#123;</span><br><span class="line">            <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(e.node_id, e.index);</span><br><span class="line">            args.<span class="built_in">push_back</span>(*(data_entry_[eid].<span class="keyword">operator</span>-&gt;()));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">uint32_t</span> index = <span class="number">0</span>; index &lt; inode.param.num_outputs; ++index) &#123;</span><br><span class="line">            <span class="keyword">uint32_t</span> eid = <span class="keyword">this</span>-&gt;<span class="built_in">entry_id</span>(nid, index);</span><br><span class="line">            args.<span class="built_in">push_back</span>(*(data_entry_[eid].<span class="keyword">operator</span>-&gt;()));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">CHECK</span>(inode.op_type == <span class="string">&quot;tvm_op&quot;</span>) &lt;&lt; <span class="string">&quot;Can only take tvm_op as op&quot;</span>;</span><br><span class="line">		<span class="comment">// 基于节点参数，输入输出缓存地址，输入通道数,创建可执行对象</span></span><br><span class="line">        op_execs_[nid] = <span class="built_in">CreateTVMOp</span>(inode.param, args, inode.inputs.<span class="built_in">size</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Node dependencies required for event wait list */</span></span><br><span class="line">    _global_depend_map_ = attrs_.depend;<span class="comment">// 更新事件等待列表</span></span><br><span class="line">    _global_event_id_.<span class="built_in">resize</span>(<span class="keyword">this</span>-&gt;<span class="built_in">GetNumOfNodes</span>(), <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="创建tvm操作算子">3.3.创建TVM操作算子</h3>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::function&lt;<span class="title">void</span><span class="params">()</span>&gt; <span class="title">GraphRuntime::CreateTVMOp</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> TVMOpParam&amp; param,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> std::vector&lt;DLTensor&gt;&amp; args,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> num_inputs)</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">OpArgs</span> &#123;</span></span><br><span class="line">    std::vector&lt;DLTensor&gt; args;</span><br><span class="line">    std::vector&lt;TVMValue&gt; arg_values;</span><br><span class="line">    std::vector&lt;<span class="keyword">int</span>&gt; arg_tcodes;</span><br><span class="line">    std::vector&lt;<span class="keyword">int64_t</span>&gt; shape_data;</span><br><span class="line">  &#125;;</span><br><span class="line">  std::shared_ptr&lt;OpArgs&gt; arg_ptr = std::make_shared&lt;OpArgs&gt;();</span><br><span class="line">  <span class="comment">// setup address.</span></span><br><span class="line">  arg_ptr-&gt;args = std::<span class="built_in">move</span>(args);<span class="comment">// 更新输入和输出的地址</span></span><br><span class="line">  <span class="comment">// 判断是否数据平铺</span></span><br><span class="line">  <span class="keyword">if</span> (param.flatten_data) &#123;</span><br><span class="line">    arg_ptr-&gt;shape_data.<span class="built_in">resize</span>(arg_ptr-&gt;args.<span class="built_in">size</span>());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 遍历输入和输出缓存区</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; arg_ptr-&gt;args.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    TVMValue v;</span><br><span class="line">    DLTensor* t = &amp;(arg_ptr-&gt;args[i]);</span><br><span class="line">    v.v_handle = t;</span><br><span class="line">    arg_ptr-&gt;arg_values.<span class="built_in">push_back</span>(v);</span><br><span class="line">    arg_ptr-&gt;arg_tcodes.<span class="built_in">push_back</span>(kArrayHandle);</span><br><span class="line">    <span class="keyword">if</span> (param.flatten_data) &#123;</span><br><span class="line">      arg_ptr-&gt;shape_data[i] = std::<span class="built_in">accumulate</span>(</span><br><span class="line">          t-&gt;shape, t-&gt;shape + t-&gt;ndim, <span class="number">1</span>, std::multiplies&lt;<span class="keyword">int64_t</span>&gt;());</span><br><span class="line">      t-&gt;ndim = <span class="number">1</span>;</span><br><span class="line">      t-&gt;shape = &amp;(arg_ptr-&gt;shape_data[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (param.func_name == <span class="string">&quot;__nop&quot;</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> []()&#123;&#125;;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (param.func_name == <span class="string">&quot;__copy&quot;</span>) &#123;</span><br><span class="line">    <span class="comment">// Perform cross device data copy.</span></span><br><span class="line">    <span class="comment">// Directly copy data from the input to the output.</span></span><br><span class="line">    <span class="keyword">auto</span> fexec = [arg_ptr]() &#123;</span><br><span class="line">      DLTensor* from = <span class="keyword">static_cast</span>&lt;DLTensor*&gt;(arg_ptr-&gt;arg_values[<span class="number">0</span>].v_handle);</span><br><span class="line">      DLTensor* to = <span class="keyword">static_cast</span>&lt;DLTensor*&gt;(arg_ptr-&gt;arg_values[<span class="number">1</span>].v_handle);</span><br><span class="line">      <span class="built_in">TVM_CCALL</span>(<span class="built_in">TVMArrayCopyFromTo</span>(from, to, <span class="literal">nullptr</span>));</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> fexec;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get compiled function from the module that contains both host and device</span></span><br><span class="line">  <span class="comment">// code.</span></span><br><span class="line">  tvm::runtime::PackedFunc pf = module_.<span class="built_in">GetFunction</span>(param.func_name, <span class="literal">false</span>);</span><br><span class="line">  <span class="built_in">CHECK</span>(pf != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;no such function in module: &quot;</span> &lt;&lt; param.func_name;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> fexec = [arg_ptr, pf]() &#123;</span><br><span class="line">    TVMRetValue rv;</span><br><span class="line">    <span class="function">TVMArgs <span class="title">targs</span><span class="params">(arg_ptr-&gt;arg_values.data(),</span></span></span><br><span class="line"><span class="params"><span class="function">                  arg_ptr-&gt;arg_tcodes.data(),</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(arg_ptr-&gt;arg_values.size()))</span></span>;</span><br><span class="line">    pf.<span class="built_in">CallPacked</span>(targs, &amp;rv);</span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">return</span> fexec;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="getfunction">3.3.1. GetFunction</h4>
<p>关于<code>module_.GetFunction</code>，根据模块不同调用不同的接口函数，常见有<code>Powervr_module.cc</code>、<code>opencl_module.cc</code>和<code>llvm_module.cc</code>等。</p>
<h5 id="powervr_module">3.3.1.1 powervr_module</h5>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">PackedFunc <span class="title">GetFunction</span><span class="params">(<span class="keyword">const</span> std::string&amp; name,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="keyword">const</span> std::shared_ptr&lt;ModuleNode&gt;&amp; sptr_to_self)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (name == runtime::symbol::tvm_module_main) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">GetFunction</span>(entry_func_, sptr_to_self);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> it = fmap_.<span class="built_in">find</span>(name);</span><br><span class="line">    <span class="keyword">if</span> (it == fmap_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::string nid_name = name;</span><br><span class="line">    nid_name.<span class="built_in">resize</span>(name.<span class="built_in">rfind</span>(<span class="string">&quot;_kernel&quot;</span>));</span><br><span class="line">    <span class="keyword">uint32_t</span> nid = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it : _global_func_nid_map_) &#123;</span><br><span class="line">        <span class="keyword">if</span> (it.second == nid_name) &#123;</span><br><span class="line">            nid = it.first;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> PowerVRWrappedFunc&amp; pf = it-&gt;second;</span><br><span class="line">    <span class="keyword">if</span> (pf.device == <span class="string">&quot;nnpu&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>([pf, sptr_to_self, <span class="keyword">this</span>, nid](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">            pf.<span class="built_in">RunNNPU</span>(args, <span class="keyword">this</span>, nid);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pf.device == <span class="string">&quot;nna&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>([pf, sptr_to_self, <span class="keyword">this</span>, nid](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">            pf.<span class="built_in">RunNNA</span>(args, <span class="keyword">this</span>, nid);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;  <span class="keyword">else</span> <span class="keyword">if</span> (pf.device == <span class="string">&quot;custompy&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>([pf, sptr_to_self, <span class="keyword">this</span>](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">            pf.<span class="built_in">RunPython</span>(args, <span class="keyword">this</span>);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;  <span class="keyword">else</span> <span class="keyword">if</span> (pf.device == <span class="string">&quot;customcpp&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>([pf, sptr_to_self, <span class="keyword">this</span>](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">            pf.<span class="built_in">RunCPP</span>(args, <span class="keyword">this</span>);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Unknown device &quot;</span> &lt;&lt; pf.device &lt;&lt; <span class="string">&quot; in PowerVRModuleNode::GetFunction function&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">PackedFunc</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>RunNNPU函数</li>
</ul>
<p>该函数获取<code>PowerVRDeviceAPI</code>设备对象，并以当前对象创建函数指针<code>pf_obj</code>对象。在设备缓存中查询是否存在该函数指针对象，如果存在，则直接执行，否则需要绑定执行函数的输入和输出内存空间，并将该函数指针对象存入内存缓存区。同时更新事件等待列表，用于函数执行后的事件等待。</p>
<p>数据绑定主要将初始化分配的内存空间绑定到节点相应的输入和输出接口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PowerVRWrappedFunc::RunNNPU</span><span class="params">(<span class="keyword">const</span> TVMArgs&amp; args, ModuleNode* mod_ctx, <span class="keyword">uint32_t</span> nid)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    std::shared_ptr&lt;tvm::runtime::PowerVRDeviceAPI&gt; dev;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;disable_wrapper == <span class="literal">false</span>) &#123;</span><br><span class="line">        PowerVRWorkspacePool *w = PowerVRWrappedFunc::<span class="built_in">PowerVRThreadLocalState</span>();</span><br><span class="line">        dev = w-&gt;<span class="built_in">GetPowerVRDevice</span>();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        NNPUWorkspacePool *w = PowerVRWrappedFunc::<span class="built_in">NNPUThreadLocalState</span>();</span><br><span class="line">        dev = w-&gt;<span class="built_in">GetNNPUDevice</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    imgdnn_err_code err;</span><br><span class="line">    <span class="keyword">unsigned</span> num_outputs = <span class="number">0</span>;</span><br><span class="line">    imgdnn_tensor* output_tensors = <span class="literal">nullptr</span>;</span><br><span class="line">    std::vector&lt;imgdnn_memory&gt; output_mems;</span><br><span class="line">    <span class="keyword">unsigned</span> num_inputs = <span class="number">0</span>;</span><br><span class="line">    imgdnn_tensor* input_tensors = <span class="literal">nullptr</span>;</span><br><span class="line">    std::vector&lt;imgdnn_memory&gt; input_mems;</span><br><span class="line">    imgdnn_network_object net_obj;</span><br><span class="line">    imgdnn_binding binding;</span><br><span class="line">    imgdnn_event e;</span><br><span class="line">    imgdnn_network network;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">ifdef</span> TVM_PVR_SCOPE</span></span><br><span class="line">    <span class="function">PowerVRScope <span class="title">scoped</span><span class="params">(dev-&gt;psComms, <span class="string">&quot;TVM Runtime NNPU device: Execute&quot;</span>)</span></span>;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> *pf_obj = <span class="keyword">const_cast</span>&lt;<span class="keyword">void</span>*&gt;(<span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">void</span>*&gt;(<span class="keyword">this</span>));</span><br><span class="line">    <span class="keyword">auto</span> it = dev-&gt;cache_.<span class="built_in">find</span>(pf_obj);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">uint32_t</span> num_events = <span class="number">0</span>;</span><br><span class="line">    std::vector&lt;DeviceEvent&gt; device_events;</span><br><span class="line">    device_events = <span class="built_in">GetWaitEventList</span>(nid, CONVERT_TO_IMGDNN);</span><br><span class="line">    num_events = device_events.<span class="built_in">size</span>();</span><br><span class="line">    std::vector&lt;imgdnn_event&gt; event_wait_list;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; num_events; i++) &#123;</span><br><span class="line">        event_wait_list.<span class="built_in">push_back</span>((imgdnn_event)device_events[i].event);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (it != dev-&gt;cache_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        PowerVRCacheEntry entry = it-&gt;second;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;RunNNPU Context @&quot;</span> &lt;&lt; dev-&gt;context_</span><br><span class="line">            &lt;&lt; <span class="string">&quot; Cached network_object @&quot;</span> &lt;&lt; entry.net_obj &lt;&lt; <span class="string">&quot; binding @&quot;</span> &lt;&lt; entry.binding</span><br><span class="line">            &lt;&lt; <span class="string">&quot; nid: &quot;</span> &lt;&lt; nid &lt;&lt; <span class="string">&quot; num_events: &quot;</span> &lt;&lt; num_events;</span><br><span class="line"></span><br><span class="line">        err = (*dev-&gt;p_NetworkObjectExecute)(entry.net_obj, entry.binding, <span class="literal">false</span>,</span><br><span class="line">                                             num_events,</span><br><span class="line">                                             num_events ? event_wait_list.<span class="built_in">data</span>() : <span class="literal">nullptr</span>,</span><br><span class="line">                                             &amp;e);</span><br><span class="line">        <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">            <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not execute NNPU network&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">SetWaitEvent</span>(nid, e, dev-&gt;context_,</span><br><span class="line">                     (<span class="keyword">void</span>*) dev-&gt;p_ImportEvent, (<span class="keyword">void</span>*) dev-&gt;p_ExportEvent,</span><br><span class="line">                     (<span class="keyword">void</span>*) dev-&gt;p_WaitForEvent, (<span class="keyword">void</span>*) dev-&gt;p_EventDestroy,</span><br><span class="line">                     <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;NNPU network execution done&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;RunNNPU Context @&quot;</span> &lt;&lt; dev-&gt;context_</span><br><span class="line">        &lt;&lt; <span class="string">&quot; nid: &quot;</span> &lt;&lt; nid &lt;&lt; <span class="string">&quot; num_events: &quot;</span> &lt;&lt; num_events;</span><br><span class="line"></span><br><span class="line">    network = (*dev-&gt;p_CreateNetworkFromIR)(<span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">void</span> *&gt;(<span class="keyword">this</span>-&gt;imgir.<span class="built_in">c_str</span>()),</span><br><span class="line">                                            (<span class="keyword">size_t</span>) <span class="keyword">this</span>-&gt;imgir.<span class="built_in">size</span>(),</span><br><span class="line">                                            <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">void</span> *&gt;(<span class="keyword">this</span>-&gt;params),</span><br><span class="line">                                            (<span class="keyword">size_t</span>) <span class="keyword">this</span>-&gt;params_size,</span><br><span class="line">                                            &amp;err);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">DumpIR</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Error: Unable to create network for NNPU device\n&quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Refer to nnpu.imgir and nnpu.imgir.params\n&quot;</span>;</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    input_tensors = (*dev-&gt;p_NetworkFindInputs)(network, &amp;num_inputs, &amp;err);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Unable to find network inputs for NNPU device&quot;</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">            <span class="keyword">const</span> TVMValue&amp; v = args.values[i + <span class="number">1</span>];</span><br><span class="line">            input_mems.<span class="built_in">push_back</span>(<span class="keyword">static_cast</span>&lt;imgdnn_memory&gt;(v.v_handle));</span><br><span class="line">            <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Input[&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;] &quot;</span> &lt;&lt; <span class="built_in">TypeCode2Str</span>(args.type_codes[i + <span class="number">1</span>]) &lt;&lt; <span class="string">&quot; @&quot;</span> &lt;&lt; v.v_handle;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    output_tensors = (*dev-&gt;p_NetworkFindDefaultOutputs)(network, &amp;num_outputs, &amp;err);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Unable to find network outputs for NNPU device&quot;</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">0</span>; i &lt; num_outputs; ++i) &#123;</span><br><span class="line">            <span class="comment">/* First output is at index 0 and remaining outputs are placed after all inputs. */</span></span><br><span class="line">            <span class="keyword">size_t</span> offset = (i == <span class="number">0</span>) ? <span class="number">0</span> : num_inputs;</span><br><span class="line">            <span class="keyword">const</span> TVMValue&amp; v = args.values[i + offset];</span><br><span class="line">            output_mems.<span class="built_in">push_back</span>(<span class="keyword">static_cast</span>&lt;imgdnn_memory&gt;(v.v_handle));</span><br><span class="line">            <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Output[&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;] &quot;</span> &lt;&lt; <span class="built_in">TypeCode2Str</span>(args.type_codes[i]) &lt;&lt; <span class="string">&quot; @&quot;</span> &lt;&lt; v.v_handle;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    net_obj = (*dev-&gt;p_CreateNetworkObject)(dev-&gt;device_[POWERVR_NNPU_DEVICE_IDX], dev-&gt;context_, network,</span><br><span class="line">                                            num_inputs, input_tensors,</span><br><span class="line">                                            num_outputs, output_tensors,</span><br><span class="line">                                            <span class="number">0</span>, <span class="string">&quot;&quot;</span>, &amp;err);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">DumpIR</span>();</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Error: Could not create network object for NNPU device\n&quot;</span>;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Refer to nnpu.imgir and nnpu.imgir.params\n&quot;</span>;</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    (*dev-&gt;p_NetworkDestroy)(network);</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;imgdnn_input&gt; <span class="title">inputs</span><span class="params">((<span class="keyword">size_t</span>)num_inputs)</span></span>;</span><br><span class="line">    err = (*dev-&gt;p_NetworkObjectGetInputs)(net_obj, num_inputs, inputs.<span class="built_in">data</span>(), <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not get inputs of NNPU network&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;imgdnn_output&gt; <span class="title">outputs</span><span class="params">((<span class="keyword">size_t</span>)num_outputs)</span></span>;</span><br><span class="line">    err = (*dev-&gt;p_NetworkObjectGetOutputs)(net_obj, num_outputs, outputs.<span class="built_in">data</span>(), <span class="literal">nullptr</span>);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not get outputs of NNPU network&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    binding = (*dev-&gt;p_CreateBinding)(&amp;err);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not create binding for NNPU network&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">0</span>; i &lt; num_inputs; ++i) &#123;</span><br><span class="line">        err = (*dev-&gt;p_BindingAddInput)(binding, inputs[i], input_mems[i]);</span><br><span class="line">        <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">            <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not add input &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; to binding of NNPU network&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">0</span>; i &lt; num_outputs; ++i) &#123;</span><br><span class="line">        err = (*dev-&gt;p_BindingAddOutput)(binding, outputs[i], output_mems[i]);</span><br><span class="line">        <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">            <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not add output &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; to binding of NNPU network&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Create cache entry */</span></span><br><span class="line">    dev-&gt;cache_[pf_obj] = <span class="built_in">PowerVRCacheEntry</span>(net_obj, binding);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Create cache entry network_object @&quot;</span> &lt;&lt; net_obj &lt;&lt; <span class="string">&quot; binding @&quot;</span> &lt;&lt; binding;</span><br><span class="line"></span><br><span class="line">    err = (*dev-&gt;p_NetworkObjectExecute)(net_obj, binding, <span class="literal">false</span>,</span><br><span class="line">                                         num_events,</span><br><span class="line">                                         num_events ? event_wait_list.<span class="built_in">data</span>() : <span class="literal">nullptr</span>,</span><br><span class="line">                                         &amp;e);</span><br><span class="line">    <span class="keyword">if</span> (err != IMGDNN_SUCCESS) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Could not execute NNPU network&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">SetWaitEvent</span>(nid, e, dev-&gt;context_,</span><br><span class="line">                 (<span class="keyword">void</span>*) dev-&gt;p_ImportEvent, (<span class="keyword">void</span>*) dev-&gt;p_ExportEvent,</span><br><span class="line">                 (<span class="keyword">void</span>*) dev-&gt;p_WaitForEvent, (<span class="keyword">void</span>*) dev-&gt;p_EventDestroy,</span><br><span class="line">                 <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">PLOG</span>(INFO) &lt;&lt; <span class="string">&quot;NNPU network execution done&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      </div>
      
      
      
    </div>

    
    


    <div class="comments gitalk-container"></div><script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Henry</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>




  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;forest&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{&quot;enable&quot;:true,&quot;github_id&quot;:&quot;zgh551&quot;,&quot;repo&quot;:&quot;hexo_gittalk&quot;,&quot;client_id&quot;:&quot;1fb8d150a53497be045f&quot;,&quot;client_secret&quot;:&quot;04fcd95e4d714c51c62222db387f2597d4b5a968&quot;,&quot;admin_user&quot;:&quot;zgh551&quot;,&quot;distraction_free_mode&quot;:true,&quot;proxy&quot;:&quot;https:&#x2F;&#x2F;cors-anywhere.herokuapp.com&#x2F;https:&#x2F;&#x2F;github.com&#x2F;login&#x2F;oauth&#x2F;access_token&quot;,&quot;language&quot;:&quot;zh-CN&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;gitalk@1.7.2&#x2F;dist&#x2F;gitalk.min.js&quot;,&quot;integrity&quot;:&quot;sha256-Pmj85ojLaPOWwRtlMJwmezB&#x2F;Qg8BzvJp5eTzvXaYAfA&#x3D;&quot;},&quot;path_md5&quot;:&quot;c886a9bdd93eea67c0eae90fd43a7b31&quot;}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
