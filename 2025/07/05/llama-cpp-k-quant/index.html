<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="1AYoSrGqW5qB6rPYnoXq_tIpkaudIK3MXrpUhL_69NI">
  <meta name="msvalidate.01" content="DF49D5E49BAD0CB8947DDD3824A370B1">
  <meta name="baidu-site-verification" content="code-EEmJnUEq0A">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zgh551.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"changyan","storage":true,"lazyload":true,"nav":null,"activeClass":"changyan"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Overview 在llama.cpp中，常用的量化方式为 K-Quant ，这是一种混合量化方式，即不同的模型类型和权重位置会采用不同的量化位宽。其核心思想是，对于对模型精度影响较大的权重，采用更高的量化位宽。本节重点探讨了其量化算法背后的数学原理，旨在从数学角度深入理解其背后的数学原理。 K-Quant Compare 根据模型量化时是否使用IMatrix，K-Quant 算法会调用不同">
<meta property="og:type" content="article">
<meta property="og:title" content="K-Quant">
<meta property="og:url" content="https://zgh551.github.io/2025/07/05/llama-cpp-k-quant/index.html">
<meta property="og:site_name" content="Henry-Z">
<meta property="og:description" content="Overview 在llama.cpp中，常用的量化方式为 K-Quant ，这是一种混合量化方式，即不同的模型类型和权重位置会采用不同的量化位宽。其核心思想是，对于对模型精度影响较大的权重，采用更高的量化位宽。本节重点探讨了其量化算法背后的数学原理，旨在从数学角度深入理解其背后的数学原理。 K-Quant Compare 根据模型量化时是否使用IMatrix，K-Quant 算法会调用不同">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/07/05/WRkAjIpLlbyEvNZ.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/kVRtlcPfmMveHoC.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/udxF7yS58iGQscI.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/zvKGkJnZ5uiTxMD.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/yTot5sFC6MIQaeX.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/fqih9K6pjlCM7XF.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/cma32uSXkh6dEoL.png">
<meta property="og:image" content="https://s2.loli.net/2025/07/06/RiPwFv5JKeqmUVX.png">
<meta property="article:published_time" content="2025-07-05T14:35:51.000Z">
<meta property="article:modified_time" content="2025-08-02T14:44:31.253Z">
<meta property="article:author" content="Henry">
<meta property="article:tag" content="llama.cpp">
<meta property="article:tag" content="llm quant">
<meta property="article:tag" content="k-quant">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/07/05/WRkAjIpLlbyEvNZ.png">


<link rel="canonical" href="https://zgh551.github.io/2025/07/05/llama-cpp-k-quant/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zgh551.github.io/2025/07/05/llama-cpp-k-quant/","path":"2025/07/05/llama-cpp-k-quant/","title":"K-Quant"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>K-Quant | Henry-Z</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ML9CRPRYFK"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ML9CRPRYFK","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?c8e65d830c8cd51e920dc7e7c0be8744"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Henry-Z</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">天道酬勤 知行合一</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">128</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">55</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">60</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-quant-compare"><span class="nav-number">2.</span> <span class="nav-text">K-Quant Compare</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-block-split"><span class="nav-number">2.1.</span> <span class="nav-text">Data Block Split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#imatrix"><span class="nav-number">2.2.</span> <span class="nav-text">IMatrix</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#importance-matrix-calculation"><span class="nav-number">2.2.1.</span> <span class="nav-text">Importance Matrix Calculation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#i-matrix%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%BA%8E%E6%8C%87%E5%AF%BC%E6%9D%83%E9%87%8D%E9%87%8F%E5%8C%96"><span class="nav-number">2.2.2.</span> <span class="nav-text">i-Matrix可以用于指导权重量化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.2.3.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#without-imatrix"><span class="nav-number">2.3.</span> <span class="nav-text">Without IMatrix</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#data-weight-calculate"><span class="nav-number">2.3.1.</span> <span class="nav-text">Data Weight Calculate</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#with-imatrix"><span class="nav-number">2.4.</span> <span class="nav-text">With IMatrix</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#data-weight-calculate-1"><span class="nav-number">2.4.1.</span> <span class="nav-text">Data Weight Calculate</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#super-block-quant"><span class="nav-number">3.</span> <span class="nav-text">Super Block Quant</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#asymmetric-quant-and-dequant"><span class="nav-number">3.1.</span> <span class="nav-text">Asymmetric Quant and Dequant</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#symmetric-quant-and-dequant"><span class="nav-number">3.2.</span> <span class="nav-text">Symmetric Quant and Dequant</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quant-optimization"><span class="nav-number">4.</span> <span class="nav-text">Quant Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#asymmetric"><span class="nav-number">4.1.</span> <span class="nav-text">Asymmetric</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-error"><span class="nav-number">4.1.1.</span> <span class="nav-text">Quant Error</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-scale-opt"><span class="nav-number">4.1.2.</span> <span class="nav-text">Quant-Scale OPT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-scale-update"><span class="nav-number">4.1.3.</span> <span class="nav-text">Quant-Scale Update</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#symmetric-quant-and-global-optimumq6_k"><span class="nav-number">4.2.</span> <span class="nav-text">Symmetric Quant and Global Optimum(Q6_K)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-error-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">Quant Error</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-scale-opt-1"><span class="nav-number">4.2.2.</span> <span class="nav-text">Quant Scale OPT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#quant-scale-update-1"><span class="nav-number">4.2.3.</span> <span class="nav-text">Quant Scale Update</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#symmetric-quant-and-local-optimumq3_k"><span class="nav-number">4.3.</span> <span class="nav-text">Symmetric Quant and Local Optimum(Q3_K)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#super-block-scale-quant"><span class="nav-number">5.</span> <span class="nav-text">Super-Block Scale Quant</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Henry"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Henry</p>
  <div class="site-description" itemprop="description">Opportunity knocks but once</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">128</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3pnaDU1MQ==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zgh551"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlbnJ5emh1NTUxQGdtYWlsLmNvbQ==" title="E-Mail → mailto:henryzhu551@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pnaGZvcmV2ZXI=" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;zghforever"><i class="fa-solid fa-c fa-fw"></i>CSDN</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL3pnaDU1MS8=" title="Linkindin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zgh551&#x2F;"><i class="fab fa-linkedin fa-fw"></i>Linkindin</span>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zgh551.github.io/2025/07/05/llama-cpp-k-quant/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Henry">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Henry-Z">
      <meta itemprop="description" content="Opportunity knocks but once">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="K-Quant | Henry-Z">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          K-Quant
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-05 22:35:51" itemprop="dateCreated datePublished" datetime="2025-07-05T22:35:51+08:00">2025-07-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-02 22:44:31" itemprop="dateModified" datetime="2025-08-02T22:44:31+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/llama-cpp/" itemprop="url" rel="index"><span itemprop="name">llama.cpp</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/llama-cpp/quant/" itemprop="url" rel="index"><span itemprop="name">quant</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    <a title="K-Quant" href="/2025/07/05/llama-cpp-k-quant/#SOHUCS" itemprop="discussionUrl">
      <span id="sourceId::135bae82957fc0a71f1f0f3f2e6aff0e" class="cy_cmt_count" itemprop="commentCount"></span>
    </a>
  </span>
  
  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2025/07/05/llama-cpp-k-quant/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2025/07/05/llama-cpp-k-quant/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="overview">Overview</h2>
<p>在<code>llama.cpp</code>中，常用的量化方式为 <strong>K-Quant</strong> ，这是一种<code>混合量化</code>方式，即不同的<strong>模型类型</strong>和<strong>权重位置</strong>会采用不同的量化<code>位宽</code>。其核心思想是，对于对模型精度影响较大的权重，采用更高的量化位宽。本节重点探讨了其量化算法背后的数学原理，旨在从数学角度深入理解其背后的数学原理。</p>
<h2 id="k-quant-compare">K-Quant Compare</h2>
<p>根据模型量化时是否使用<strong>IMatrix</strong>，<code>K-Quant</code> 算法会调用不同的实现，详细的对比如下：</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/05/WRkAjIpLlbyEvNZ.png" alt="Compare K-Quant" /><figcaption aria-hidden="true">Compare K-Quant</figcaption>
</figure>
<span id="more"></span>
<h3 id="data-block-split">Data Block Split</h3>
<p><strong>K-Quant</strong> 采用<code>逐块量化</code>的方式，首先从原始权重数据中取出<code>256</code>个元素作为一个<strong>量化Block</strong>，然后再将其分割为<code>8</code>个<strong>Super-Block</strong>，每个<strong>Super-Block</strong>包含<code>32</code>个待量化的权重数据。</p>
<ul>
<li><span class="math inline">\(N\)</span>：表示<code>Block</code>的大小；</li>
<li><span class="math inline">\(n\)</span>：表示<code>Super-Block</code>的大小；</li>
</ul>
<h3 id="imatrix">IMatrix</h3>
<p><strong>K-Quant</strong> 支持使用<code>i-matrix</code>进一步提升量化精度，其核心思想是<strong>利用激活值 (activations) 的统计信息来指导权重(weights)量化</strong>。通常的量化方法可能会忽略激活值的影响，该方法试图根据激活值的<strong>重要性</strong>来调整量化系数，从而使得量化的最终目标(量化误差最小)达到最优。 对于<code>i-matrix</code>概念的理解，重点在于<strong>Importance</strong>单词，表示对量化精度影响比较<strong>重要</strong>的权重。当量化时，准备一份校准数据集，通过前向推理收集网络每层的激活值信息。然后根据这些激活值的分布，计算出权重对于量化目标的敏感度，哪些更重要，哪些不那么重要。</p>
<h4 id="importance-matrix-calculation">Importance Matrix Calculation</h4>
<p>如何计算出 <code>i-matrix</code>，其背后的数学原理推理如下：</p>
<p>假设网络某一层的<strong>权重向量</strong>表示为：</p>
<p><span class="math display">\[
W_{m\times n}=
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; ... &amp; w_{1,n}\\
w_{2,1} &amp; w_{2,2} &amp; ... &amp; w_{2,n}\\
... &amp; ... &amp; ... &amp; ...\\
w_{m,1} &amp; w_{m,2} &amp; ... &amp; w_{m,n}\\
\end{bmatrix}
\]</span></p>
<p>其中， <span class="math inline">\(m\)</span>表示行， <span class="math inline">\(n\)</span>表示列，则第 <span class="math inline">\(i\)</span> 行的<strong>权重向量</strong>为</p>
<p><span class="math display">\[
W_{i,:}=\mathbf{w}_i^T =
\begin{bmatrix}
w_{i,1} &amp;w_{i,2}&amp;...&amp;w_{i,n}
\end{bmatrix}
\]</span></p>
<p>假设网络某一层的<strong>激活输入</strong>为</p>
<p><span class="math display">\[
X_{n\times k}=
\begin{bmatrix}
x_{1,1} &amp; x_{1,2} &amp; ... &amp; x_{1,k}\\
x_{2,1} &amp; x_{2,2} &amp; ... &amp; x_{2,k}\\
... &amp; ... &amp; ... &amp; ...\\
x_{n,1} &amp; x_{n,2} &amp; ... &amp; x_{n,k}\\
\end{bmatrix}
\]</span></p>
<p>其中， <span class="math inline">\(n\)</span> 表示行， <span class="math inline">\(k\)</span> 表示列，则第 <span class="math inline">\(j\)</span> 列的<strong>激活向量</strong>为：</p>
<p><span class="math display">\[
X_{:,j}=\mathbf{x}_j=
\begin{bmatrix}
x_{1,j}\\
x_{2,j}\\
...\\
x_{n,j}
\end{bmatrix}=(x_{1,j},x_{2,j},...,x_{n,j})
\]</span></p>
<p>那么神经网络层中的一个输出可以表示为<code>权重向量</code> <span class="math inline">\(\mathbf{w}\)</span> 和<code>激活向量</code> <span class="math inline">\(\mathbf{x}\)</span> 的<strong>点积</strong>：</p>
<p><span class="math display">\[y=\mathbf{w}_i^T\cdot \mathbf{x}_j=\sum_{t=1}^n w_{i,t}*x_{t,j}\tag{1}\]</span></p>
<p>假设<code>权重向量</code> <span class="math inline">\(\mathbf{w}\)</span> 的<strong>量化形式</strong>表示为</p>
<p><span class="math display">\[
\mathbf{q}_i^T =
\begin{bmatrix}
q_{i,1} &amp;q_{i,2}&amp;...&amp;q_{i,n}
\end{bmatrix}
\]</span></p>
<p>权重量化后的输出可以表示为</p>
<p><span class="math display">\[y_q=\mathbf{q}_i^T\cdot \mathbf{x}_j=\sum_{t=1}^n q_{i,t}*x_{t,j} \tag{2}\]</span></p>
<p>最终量化目标是使量化后的权重 <code>q</code> 和原始权重 <code>w</code> 与激活向量 <span class="math inline">\(a\)</span> 的<strong>点积</strong>结果尽量接近，即最小化量化引入的误差，以最小化<code>RMSE</code>为<strong>目标函数</strong>表示如下：</p>
<p><span class="math display">\[\mathtt{min} \quad |y_q-y|^2\]</span></p>
<p>结合公式(1)和(2)得，<strong>误差函数</strong>可以表示如下：</p>
<p><span class="math display">\[F_{i,j}=\Bigg[\sum_{t=1}^n(q_{i,t}-w_{i,t})x_{t,j}\Bigg]^2 \tag{3}\]</span></p>
<p>定义每个元素的<strong>量化误差</strong>为 <span class="math inline">\(r_{i,t}=q_{i,t}-w_{i,t}\)</span> ，则</p>
<p><span class="math display">\[F_{i,j}=\Bigg[\sum_{t=1}^n r_{i,t}x_{t,j}\Bigg]^2 \tag{4}\]</span></p>
<p>由于上述公式(4)只考虑了第 <span class="math inline">\(i\)</span> 行权重和第 <span class="math inline">\(j\)</span> 列激活值引起的<strong>量化误差</strong>，故可以考虑整个网络的误差表示为</p>
<p><span class="math display">\[
F=\begin{bmatrix}
F_{1,1} &amp; F_{1,2} &amp;... &amp;F_{1,k}\\
F_{2,1} &amp; F_{2,2} &amp;... &amp;F_{2,k}\\
... &amp; ... &amp; ... &amp; ...\\
F_{m,1} &amp; F_{m,2} &amp;... &amp;F_{m,k}\\
\end{bmatrix}
\tag{5}
\]</span></p>
<p>等效于将 <span class="math inline">\(F\)</span> 的期望值 <span class="math inline">\(\mathbf{E}[F]\)</span> 作为量化目标：</p>
<p><span class="math display">\[\large \mathbf{E}[F]=\mathbf{E}\Bigg[\Big(\sum_{t=1}^nx_tr_t\Big)^2\Bigg]\tag{6}\]</span></p>
<div class="note info"><p>假设 <span class="math inline">\(g = \sum_i^3x_i=x_1 + x_2 + x_3\)</span> 则: <span class="math display">\[\begin{align}
g^2
&amp;= \Big(\sum_i^3x_i\Big)^2\\
&amp;= (x_1 + x_2 + x_3)(x_1 + x_2 + x_3)\\
&amp;=x_1(x_1 + x_2 + x_3)+x_2(x_1 + x_2 + x_3)+x_3(x_1 + x_2 + x_3)\\
&amp;=\sum_i^3\sum_j^3x_ix_j
\end{align}
\tag{7}\]</span></p>
</div>
<p>结合上述<code>Tips</code>，将表达式(6)展开得</p>
<p><span class="math display">\[
\begin{align}
\mathbf{E}\Bigg[\Big(\sum_{t=1}^nx_tr_t\Big)^2\Bigg]
&amp;=\mathbf{E}\Bigg[ \Big(\sum_{t=1}^nx_tr_t\Big)\Big(\sum_{s=1}^nx_sr_s\Big)\Bigg]\\
&amp;=\mathbf{E}\Bigg[ \sum_{t=1}^n\sum_{s=1}^n(x_tr_t)(x_sr_s)\Bigg]\\
&amp;=\mathbf{E}\Bigg[ \sum_{t=1}^n\sum_{s=1}^n(r_tr_s)(x_tx_s)\Bigg]\\
\end{align}
\tag{8}
\]</span></p>
<p>由于量化误差 <span class="math inline">\(r\)</span> 由权重量化过程确定，与输入激活 <span class="math inline">\(x\)</span> 无关，可以将 <span class="math inline">\(r\)</span> 移出期望运算符，故公式(8)可进一步化简得</p>
<p><span class="math display">\[
\large\mathbf{E}[F]=\mathbf{E}\Bigg[ \sum_{t=1}^n\sum_{s=1}^n(r_tr_s)(x_tx_s)\Bigg]=\sum_{t=1}^n\sum_{s=1}^n(r_tr_s)\mathbf{E}(x_tx_s)
\tag{9}\]</span></p>
<p>公式(9)中出现了 <span class="math inline">\(\mathbf{E}(x_tx_s)\)</span> 项，表示输入激活 <span class="math inline">\(\mathbf{x}_j\)</span> 中第 <span class="math inline">\(t\)</span> 个元素和第 <span class="math inline">\(s\)</span> 个元素的协方差，可以定义矩阵 <span class="math inline">\(\mathbf{I}\)</span>为：</p>
<p><span class="math display">\[\mathbf{I}=
\begin{bmatrix}
\mathbf{E}(x_1x_1) &amp; \mathbf{E}(x_1x_2) &amp; ... &amp; \mathbf{E}(x_1x_n)\\
\mathbf{E}(x_2x_1) &amp; \mathbf{E}(x_2x_2) &amp; ... &amp; \mathbf{E}(x_2x_n)\\
... &amp; ... &amp; ... &amp; ...\\
\mathbf{E}(x_nx_1) &amp; \mathbf{E}(x_nx_2) &amp; ... &amp; \mathbf{E}(x_nx_n)\\
\end{bmatrix}
\tag{10}\]</span></p>
<p>其中的某个元素可以表示为：</p>
<p><span class="math display">\[\mathbf{I}_{t,s}=\mathbf{E}(x_tx_s)\]</span></p>
<p>将期望符号转换为求和符号得</p>
<p><span class="math display">\[\mathbf{I}_{t,s}=\frac{1}{k}\sum_{j=1}^{k}x_{t,j}x_{s,j}\]</span></p>
<p>则矩阵 <span class="math inline">\(\mathbf{I}\)</span> 的计算可以表示为</p>
<p><span class="math display">\[\mathbf{I}=\frac{1}{k}\sum_{j=1}^{k}\mathbf{x}_j\mathbf{x}_j^T=\frac{1}{k}\mathbf{X}\mathbf{X}^T\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{x}_j= \begin{bmatrix} x_{1,j}\\ x_{2,j}\\ ...\\ x_{n,j} \end{bmatrix}\)</span> 。在实际计算中，通常会忽略归一化因子 <span class="math inline">\(\frac{1}{k}\)</span>。</p>
<p>将公式(9)拆分为对角元素和非对角元素两部分得</p>
<p><span class="math display">\[\large\mathbf{E}[F]=\sum_{i=j}\mathbf{E}[x_j^2]r_j^2+\sum_{i\neq j}\mathbf{E}[x_ix_j]r_ir_j
\tag{11}\]</span></p>
<p>由于激活值的<code>正负性</code>以及非对角元素的激活值之间相关性<code>较弱</code>，故非对角线元素 <span class="math inline">\(\mathbf{E}[x_{i}x_{j}]\)</span> 的期望值通常<code>远小于</code>对角线元素 <span class="math inline">\(\mathbf{E}[x_j^2]\)</span> ，即</p>
<p><span class="math display">\[\mathbf{E}[x_ix_j] \lll \mathbf{E}[x_j^2]\]</span></p>
<p>因此可以<strong>忽略</strong><code>非对角线元素</code>，简化计算，只关注对角线元素，即激活值的平方期望 <span class="math inline">\(\mathbf{E}[x_j^2]\)</span> ，故公式(11)忽略非对角线项得</p>
<p><span class="math display">\[\large\mathbf{E}[F]\approx \sum_{i=j}\mathbf{E}[x_j^2]r_j^2\]</span></p>
<h4 id="i-matrix可以用于指导权重量化"><code>i-Matrix</code>可以用于指导权重量化</h4>
<p>因此，最小化权重的 <span class="math inline">\(\sum_{i=j}\mathbf{E}[x_i^2](q_i-\mathtt{w}_i)^2\)</span> 等效于最小化 <span class="math inline">\(\mathbf{E}[F]\)</span> ，表达式 <span class="math inline">\(\mathbf{E}[x_i^2]\)</span> 可以作为 <strong>Importance Matrix</strong> 去指导量化 <span class="math inline">\(\mathtt{w}\)</span> 权重矩阵。当 <span class="math inline">\(\mathbf{E}[x_i^2]\)</span> 激活项较大时，需要减少对应的量化误差 <span class="math inline">\(r_i^2\)</span>， 也就是该权重要重点量化，这样才能保证目标函数 <span class="math inline">\(\mathbf{E}[F]\)</span> <strong>CBN XF</strong>。</p>
<h4 id="代码示例">代码示例</h4>
<p>在<code>llama.cpp</code>中计算 <code>imatrix</code> 矩阵的计算代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> row = <span class="number">0</span>; row &lt; (<span class="type">int</span>)src1-&gt;ne[<span class="number">1</span>]; ++row) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> * x = data + row * src1-&gt;ne[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; (<span class="type">int</span>)src1-&gt;ne[<span class="number">0</span>]; ++j) &#123;</span><br><span class="line">        e.values[j] += x[j]*x[j]; <span class="comment">// 核心计算代码</span></span><br><span class="line">        e.counts[j]++;</span><br><span class="line">        <span class="keyword">if</span> (!std::<span class="built_in">isfinite</span>(e.values[j])) &#123;</span><br><span class="line">            <span class="built_in">LOG_ERR</span>(<span class="string">&quot;%f detected in %s\n&quot;</span>, e.values[j], wname.<span class="built_in">c_str</span>());</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="without-imatrix">Without IMatrix</h3>
<h4 id="data-weight-calculate">Data Weight Calculate</h4>
<ol type="1">
<li><strong>数据块(局部)幅度</strong></li>
</ol>
<p>首先计算权重数据块的局部(32)的平均幅度，即数据块的<strong>均方根(Root Mean Square-RMS)</strong>，公式如下： <span class="math display">\[\mathbf{E}[X^2]=\sqrt\frac{\sum_{i=1}^{n}x_i^2}{n}\]</span> 这部分是块内所有数据点共享的，反映了整个数据块的平均能量水平。如果一个数据块的 <code>av_x</code> 很大，那么这个块内的所有数据点都会获得一个较高的基础权重。</p>
<ol start="2" type="1">
<li><strong>数据点幅度</strong></li>
</ol>
<p>通过计算当前数据点的绝对值，获得每个数据个体的幅度值，用于反应数据本身的大小，公式如下： <span class="math display">\[|x_i|\]</span> 这部分是每个数据独有的，反映了该数据点自身的绝对大小。绝对值越大的数据点，会获得额外的权重。</p>
<ol start="3" type="1">
<li><strong>数据点权重计算</strong></li>
</ol>
<p>每个数据点的权重为<strong>数据块幅度</strong>和<strong>数据点幅度</strong>两部分之和，最终的权重计算如下： <span class="math display">\[W_i=|x_i|+\mathbf{E}[X^2]\]</span> 在后续的量化优化过程中，具有以下特点的数据，其量化误差将会被赋予更高的权重(更重要)：</p>
<ul>
<li><strong>位于“幅度较高”的局部邻域内的数据点</strong>：如果一个数据点周围的其他数据点的幅度也较大（导致较高的数据块幅度），那么即使它自身的幅度不是特别大，也会获得较高的权重。这可以理解为，在一个重要的局部区域内，所有数据点都应该被更精确地量化。</li>
<li><strong>自身幅度较大的数据点</strong>:无论其周围邻域的能量如何，幅度较大的数据点本身就会获得较高的权重。这符合直觉，因为较大的数值的量化误差对整体的影响通常更大，因此需要更精确的表示。</li>
</ul>
<h3 id="with-imatrix">With IMatrix</h3>
<h4 id="data-weight-calculate-1">Data Weight Calculate</h4>
<ol type="1">
<li><strong>数据块峰值幅度</strong></li>
</ol>
<p>首先计算整个数据块(256)的平均幅度，即数据块的均方值(Mean Square-MS)，或者叫<strong>峰值幅度</strong>，公式如下： <span class="math display">\[\frac{2\sum_{i=1}^{N}x_i^2}{N}\]</span> 其中， <span class="math inline">\(N=256\)</span>表示整个 <code>Block</code> 的大小。</p>
<ol start="2" type="1">
<li><strong>数据点幅度</strong></li>
</ol>
<p>通过计算当前数据点的绝对值，获得每个数据个体的幅度值，用于反应数据本身的大小，公式如下： <span class="math display">\[x_i^2\]</span> 这部分是每个数据独有的，反映了该数据点自身的绝对大小。绝对值越大的数据点，会获得额外的权重。</p>
<ol start="3" type="1">
<li><strong>数据点权重计算</strong></li>
</ol>
<p>每个数据点的权重为<strong>数据块幅度</strong>和<strong>数据点幅度</strong>两部分之和，最终的权重计算如下： <span class="math display">\[W_i=w_i\times \sqrt{x_i^2+\frac{2\sum_{i=1}^{N}x_i^2}{N}}\]</span> 其中， <span class="math inline">\(w_i\)</span>表示<code>imatrix</code>中的权重。</p>
<h2 id="super-block-quant">Super Block Quant</h2>
<h3 id="asymmetric-quant-and-dequant">Asymmetric Quant and Dequant</h3>
<p>根据数据分布范围，找出其中的<strong>最大值</strong>和<strong>最小值</strong>，然后根据量化的bit数(例如4bit)，按照如下公式计算量化的缩放系数： <span class="math display">\[\delta=\frac{f_{max}-f_{min}}{2^{bit}-1}=\frac{f_{max}-f_{min}}{15}\]</span></p>
<p>如下图所示，是4bit非对称量化的示意图，显示了浮点数值分布与量化数值分布的关系：</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/kVRtlcPfmMveHoC.png" alt="4bit asymmetric quant" /><figcaption aria-hidden="true">4bit asymmetric quant</figcaption>
</figure>
<p><strong>量化</strong>表示如下：</p>
<p><span class="math display">\[q=\frac{f-f_{min}}{\delta}\]</span></p>
<p><strong>反量化</strong>表示如下：</p>
<p><span class="math display">\[f = q\times\delta+ f_{min}\]</span></p>
<h3 id="symmetric-quant-and-dequant">Symmetric Quant and Dequant</h3>
<p>如果是对称量化，计算比较简单，直接找出数据分布的最大值，然后根据量化bit位宽，按照如下公式计算量化系数：</p>
<p><span class="math display">\[\delta=\frac{|f|_{max}}{-2^{(bit-1)}}\]</span> 其中， <span class="math display">\[|f|_{max}=
\begin{cases}
   f_{max} &amp;\text{if } |f_{max}|&gt;|f_{min}| \\
   f_{min} &amp;\text{if } |f_{max}|&lt;|f_{min}|
\end{cases}\]</span></p>
<p>如下图所示，是6bit对称量化的示例图，根据绝对值最大的值是负数还是正数，采取不同的映射策略：</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/udxF7yS58iGQscI.png" alt="6bit symmetric quant" /><figcaption aria-hidden="true">6bit symmetric quant</figcaption>
</figure>
<p><strong>量化</strong>表示如下：</p>
<p><span class="math display">\[q=\frac{f}{\delta}\]</span></p>
<p><strong>反量化</strong>表示如下：</p>
<p><span class="math display">\[f = q\times\delta\]</span></p>
<h2 id="quant-optimization">Quant Optimization</h2>
<p>根据采用的量化算法的对称方式，可以分为<strong>对称</strong>和<strong>非对称</strong>两种方式，它们详细的量化误差表示略有不同，下面根据不同量化对称方式分析其量化算法的数学原理。</p>
<h3 id="asymmetric">Asymmetric</h3>
<p>如下图所示，是目前<code>llama.cpp</code>采用<strong>非对称量化</strong>算法的数据结构示意图，主要分析了<code>Block</code>内的浮点和量化数据是如何在内存中排布的。其中图的左边表示浮点数据的排布，右边的<code>qs</code>或<code>qh</code>表示量化数据的排布。</p>
<ul>
<li><strong>Q2_K</strong></li>
</ul>
<p>如下图所示，是 <code>Q2_K</code> 量化算法的数据存储示意图。整个 <strong>Block</strong> 的<strong>量化系数</strong>和<strong>零点值</strong>以<code>float16</code>格式存储，占据<code>4</code>字节的空间。然后再将整个浮点数据块 <strong>Block</strong> 划分为<code>16</code>个 <strong>Super-Block</strong> ，每个 <strong>Super-Block</strong> 拥有自己的<strong>量化系数</strong>和<strong>零点值</strong>，且各自的数据位宽为<code>4bit</code>，正好拼接为<code>1byte</code>存储，<code>高4bit</code>表示<strong>零点值</strong>，<code>低4bit</code>位表示<strong>量化系数</strong>，一共占据16个字节空间。其中，红色的Block0和Block1占据整个32个量化qs的最低2bit，其它Block按照图中规律存储。</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/zvKGkJnZ5uiTxMD.png" alt="Q2_K" /><figcaption aria-hidden="true">Q2_K</figcaption>
</figure>
<ul>
<li><strong>Q4_K</strong></li>
</ul>
<p>如下图所示，是 <code>Q4_K</code> 量化算法的数据存储示意图。整个<strong>Block</strong> 的<strong>量化系数</strong>和<strong>零点值</strong>以<code>float16</code>格式存储，占据<code>4</code>字节的空间。然后再将整个浮点数据块 <strong>Block</strong> 划分为<code>8</code>个 <strong>Super-Block</strong> ，每个 <strong>Super-Block</strong> 拥有自己的<strong>量化系数</strong>和<strong>零点值</strong>，且数据位宽为<code>6bit</code>，按照图中的结构进行拼接，一共占据<code>12</code>个字节空间。每两个 <strong>Super-Block</strong> 进行拼接，分别占据每个<code>qs</code>对象的高和低4bit，并使用一个<code>32</code>字节的数组保存，一共<strong>4</strong>个组共<code>128</code>字节。</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/yTot5sFC6MIQaeX.png" alt="Q4_K" /><figcaption aria-hidden="true">Q4_K</figcaption>
</figure>
<ul>
<li><strong>Q5_k</strong></li>
</ul>
<p>如下图所示，是 <code>Q5_K</code> 量化算法的数据存储示意图。整个 <strong>Block</strong> 的<strong>量化系数</strong>和<strong>零点值</strong>以<code>float16</code>格式存储，占据4字节的空间。然后再将 <strong>Block</strong> 划分为<code>8</code>个 <strong>Super-Block</strong> ，每个 <strong>Super-Block</strong> 拥有自己的<strong>量化系数</strong>和<strong>零点值</strong>，且数据位宽为<code>6bit</code>，按照图中的结构进行拼接，一共占据<code>12</code>个字节空间。每两个 <strong>Super-Block</strong> 的低4bit位进行合并，组成一个<code>32</code>字节的<code>qs</code>数组。这8个<strong>Super-Block</strong>的第5个bit拼接为一个字节，使用一个32字节的<code>qh</code>数组保存，一共<code>160</code>字节。</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/fqih9K6pjlCM7XF.png" alt="Q5_K" /><figcaption aria-hidden="true">Q5_K</figcaption>
</figure>
<h4 id="quant-error">Quant Error</h4>
<p>为了最小化整个数据块的量化误差，可以将某个数据的量化误差表示为： <span class="math display">\[Q(i)_{err}=|q_{i}\times\delta+ f_{min}-f_i|\]</span> 或者 <span class="math display">\[Q(i)_{err}=(q_{i}\times\delta+ f_{min}-f_i)^2\]</span> 则整个<strong>Super-Block</strong>内的量化误差表示为： <span class="math display">\[Q_{err}=\sum_{i=1}^{n}Q(i)_{err}\]</span> 由于每个数据对量化误差的敏感度不一，则将考虑了<strong>误差权重</strong>的表达式表示如下： <span class="math display">\[Q_{err}=\sum_{i=1}^{n}W_{i}\times Q(i)_{err}\]</span></p>
<h4 id="quant-scale-opt">Quant-Scale OPT</h4>
<p>为了最小化<strong>Super-Block</strong>内的量化误差之和，则设定优化目标可以表示为：</p>
<p><span class="math display">\[\mathbf{argmin}_{\delta,f_{min}} \quad Q_{err}=\sum_{i=1}^{n}W_i \times (q_{i}\times\delta+ f_{min}-f_i)^2\]</span></p>
<p>可以通过求解最小二乘问题，求得优化的 <span class="math inline">\(\delta\)</span>和 <span class="math inline">\(f_{min}\)</span>。</p>
<ol type="1">
<li>对 <span class="math inline">\(\delta\)</span>求偏导，并令等式为<code>0</code>得： <span class="math display">\[\sum_{i=1}^{n}W_i \times (q_{i}\times\delta+ f_{min}-f_i)q_i=0\]</span> 上式展开得 <span class="math display">\[\delta\sum_{i=1}^{n}W_{i}\times q_{i}^2 +f_{min}\sum_{i=1}^{n}W_iq_i=\sum_{i=1}^{n}W_{i}f_iq_i \tag{1}\]</span></li>
<li>对 <span class="math inline">\(f_{min}\)</span>求偏导，并令等式为<code>0</code>得： <span class="math display">\[\sum_{i=1}^{n}W_i \times (q_{i}\times\delta+ f_{min}-f_i)=0\]</span> 上式展开得 <span class="math display">\[\delta\sum_{i=1}^{n}W_{i}\times q_{i} +f_{min}\sum_{i=1}^{n}W_i=\sum_{i=1}^{n}W_{i}f_i \tag{2}\]</span> 故结合上述等式(1)和(2)，可以得到两个关于 <span class="math inline">\(\delta\)</span>和 <span class="math inline">\(f_{min}\)</span>的线性方程组： <span class="math display">\[
\begin{align}
\delta\sum_{i=1}^{n}W_{i}\times q_{i}^2 +f_{min}\sum_{i=1}^{n}W_iq_i &amp;=\sum_{i=1}^{n}W_{i}f_iq_i\\
\delta\sum_{i=1}^{n}W_{i}\times q_{i}+f_{min}\sum_{i=1}^{n}W_i&amp; =\sum_{i=1}^{n}W_{i}f_i
\end{align}
\tag{3}
\]</span> 将等式(3)使用矩阵形式表示如下： <span class="math display">\[
\Large
\begin{bmatrix}
\sum_{i=1}^{n}W_{i}q_{i}^2 &amp; \sum_{i=1}^{n}W_iq_i \\
\sum_{i=1}^{n}W_{i}q_{i} &amp; \sum_{i=1}^{n}W_i
\end{bmatrix}
\begin{bmatrix}
\delta\\
f_{min}
\end{bmatrix}=
\begin{bmatrix}
\sum_{i=1}^{n}W_{i}f_iq_i\\
\sum_{i=1}^{n}W_{i}f_i
\end{bmatrix}
\tag{4}
\]</span></li>
</ol>
<div class="note success"><p><strong>克莱姆法则</strong>: 假设多项式表示如下： <span class="math display">\[
\begin{array}{cc}
ax+by&amp;=\textcolor{red}{e}\\
cx+dy&amp;=\textcolor{red}{f}
\end{array}
\]</span> 使用矩阵来表示为： <span class="math display">\[
\begin{bmatrix}
a &amp; b\\
c &amp; d
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}=
\begin{bmatrix}
\textcolor{red}{e}\\\textcolor{red}{f}
\end{bmatrix}
\]</span> 当矩阵可逆时，x和y可以从克莱姆法则中得出： <span class="math display">\[
x=\frac{
\begin{vmatrix}
\textcolor{red}{e} &amp; b\\
\textcolor{red}{f} &amp; d
\end{vmatrix}
}{
\begin{vmatrix}
a &amp; b\\
c &amp; d
\end{vmatrix}
}=\frac{\textcolor{red}{e}d-b\textcolor{red}{f}}{ad-bc}
\]</span> 同时 <span class="math display">\[
y=\frac{
\begin{vmatrix}
a&amp;\textcolor{red}{e}\\
c&amp;\textcolor{red}{f}
\end{vmatrix}
}{
\begin{vmatrix}
a &amp; b\\
c &amp; d
\end{vmatrix}
}=\frac{a\textcolor{red}{f}-\textcolor{red}{e}c}{ad-bc}
\]</span> 假设使用 <span class="math inline">\(\mathbf{A}= \begin{bmatrix} a &amp; b\\ c &amp; d \end{bmatrix}\)</span>表示线性矩阵，则行列式表示为<span class="math inline">\(\det(\mathbf{A})=\begin{vmatrix} a &amp; b\\ c &amp; d \end{vmatrix}\)</span></p>
<ul>
<li><strong>可解性判据</strong>: <span class="math inline">\(\det(\mathbf{A})\)</span> 是用于判断上述线性方程组是否存在唯一解的关键。
<ol type="1">
<li>如果 <span class="math inline">\(\det(\mathbf{A})&gt;0\)</span> ，则方程组有唯一解，意味着我们可以找到一组唯一的 <span class="math inline">\(\delta\)</span> 和 <span class="math inline">\(f_{min}\)</span> 来最小化误差。</li>
<li>如果 <span class="math inline">\(\det(\mathbf{A})=0\)</span>，则方程组无解或有无穷多解，这意味着我们无法找到唯一的 <span class="math inline">\(\delta\)</span> 和 <span class="math inline">\(f_{min}\)</span> 来最优地拟合数据。这通常发生在 的所有值都相等或存在线性依赖关系的情况下。</li>
<li>如果 <span class="math inline">\(\det(\mathbf{A})&lt;0\)</span>，在实数域中通常不会出现这种情况，因为 <span class="math inline">\(ad \ge bc\)</span>（这可以从柯西-施瓦茨不等式推导出来）。</li>
</ol></li>
</ul>
</div>
<p>根据<strong>克莱姆法则</strong>，最优的 <span class="math inline">\(\delta\)</span>可以表示为： <span class="math display">\[\delta=\frac{
\begin{vmatrix}
\textcolor{red}{\sum_{i=1}^{n}W_{i}f_iq_i} &amp; \sum_{i=1}^{n}W_iq_i\\
\textcolor{red}{\sum_{i=1}^{n}W_{i}f_i} &amp; \sum_{i=1}^{n}W_i
\end{vmatrix}
}{
\begin{vmatrix}
\sum_{i=1}^{n}W_{i}q_{i}^2 &amp; \sum_{i=1}^{n}W_iq_i \\
\sum_{i=1}^{n}W_{i}q_{i} &amp; \sum_{i=1}^{n}W_i
\end{vmatrix}
}=\frac{
\textcolor{red}{\sum_{i=1}^{n}W_{i}f_iq_i} \times \sum_{i=1}^{n}W_i-\sum_{i=1}^{n}W_iq_i\times \textcolor{red}{\sum_{i=1}^{n}W_{i}f_i}
}{\sum_{i=1}^{n}W_{i}q_{i}^2\times \sum_{i=1}^{n}W_i-\sum_{i=1}^{n}W_iq_i\times \sum_{i=1}^{n}W_iq_i}
\tag{5}\]</span> 同理，最优的 <span class="math inline">\(f_{min}\)</span>可以表示为 <span class="math display">\[f_{min}=\frac{
\begin{vmatrix}
\sum_{i=1}^{n}W_{i}q_{i}^2 &amp;\textcolor{red}{\sum_{i=1}^{n}W_{i}f_iq_i}\\
\sum_{i=1}^{n}W_{i}q_{i} &amp; \textcolor{red}{\sum_{i=1}^{n}W_{i}f_i}
\end{vmatrix}
}{
\begin{vmatrix}
\sum_{i=1}^{n}W_{i}q_{i}^2 &amp; \sum_{i=1}^{n}W_iq_i \\
\sum_{i=1}^{n}W_{i}q_{i} &amp; \sum_{i=1}^{n}W_i
\end{vmatrix}
}=\frac{
\sum_{i=1}^{n}W_{i}q_{i}^2 \times \textcolor{red}{\sum_{i=1}^{n}W_{i}f_i}-\textcolor{red}{\sum_{i=1}^{n}W_{i}f_iq_i}\times \sum_{i=1}^{n}W_{i}q_{i}
}{
\sum_{i=1}^{n}W_{i}q_{i}^2\times \sum_{i=1}^{n}W_i-\sum_{i=1}^{n}W_iq_i\times \sum_{i=1}^{n}W_iq_i
}
\tag{6}\]</span> 其中，需要保证 <span class="math inline">\(\begin{vmatrix} \sum_{i=1}^{n}W_{i}q_{i}^2 &amp; \sum_{i=1}^{n}W_iq_i \\ \sum_{i=1}^{n}W_{i}q_{i} &amp; \sum_{i=1}^{n}W_i \end{vmatrix} &gt; 0\)</span>。</p>
<h4 id="quant-scale-update">Quant-Scale Update</h4>
<p>根据量化公式 <span class="math inline">\(q=\frac{f-f_{min}}{\delta}\)</span> 知，通过动态调整量化系数 <span class="math inline">\(\delta\)</span> 的大小，可以对数据的量化结果进行调整。</p>
<p><span class="math display">\[\set{\frac{1}{\delta_j}|\frac{(2^{bit}-1)+r_{min}+\delta_r*j}{f_{max}-f_{min}}, j \in[1,step]}\]</span></p>
<p>假设采用<code>4bit</code>量化， <span class="math inline">\(r_{min}=-1.0\)</span>， <span class="math inline">\(\delta_r=0.1\)</span>， <span class="math inline">\(step=20\)</span>，则</p>
<p><span class="math display">\[\set{\frac{1}{\delta_j}|\frac{15-1+0.1*j}{f_{max}-f_{min}}, j \in[1,20]}\]</span></p>
<p>从而得到一组新的量化数值 <span class="math inline">\(\set{q_i|i\in[1,n]}\)</span> ，根据量化的最优目标 <span class="math inline">\(\mathtt{minimine} \quad Q_{err}\)</span> ，并结合<code>公式(5)和(6)</code>可以得到新的量化系数 <span class="math inline">\(\delta_{b}\)</span> 和 <span class="math inline">\(b_{min}\)</span>，并重新计算出新的 <span class="math inline">\(Q_{err}^j=\sum_{i=1}^{n}W_i \times (q_{i}\times\delta_{b}+ b_{min}-f_i)^2\)</span></p>
<p>如果新的 <span class="math inline">\(Q_{err}^j &lt; Q_{err}\)</span> ，则更新 <span class="math inline">\(\delta_{b}\)</span> 和 <span class="math inline">\(b_{min}\)</span> 为最终输出的量化系数。</p>
<h3 id="symmetric-quant-and-global-optimumq6_k">Symmetric Quant and Global Optimum(Q6_K)</h3>
<p>如下图所示，是<strong>对称量化</strong>算法的数据结构示意图，其中图的左边表示浮点数据的排布，右边的<code>qs</code>和<code>qh</code>表示量化数据的排布。下图是 <code>Q6_K</code> 量化算法的数据存储示意图，整个 <strong>Block</strong> 的<strong>量化系数</strong>以<code>float16</code>格式存储，占据<code>2</code>字节的空间。然后再将整个浮点数据块 <strong>Block</strong> 划分为<code>16</code>个 <strong>Super-Block</strong> ，每个 <strong>Super-Block</strong> 拥有自己的<strong>量化系数</strong>，且量化数据的位宽为<code>6bit</code>，一共占据<code>12</code>个字节空间。如图中红框标注的，<code>Block1-0</code>和<code>Block1-1</code>共同占据32个存储量化<code>qs</code>对象的最低<strong>4bit</strong>，<code>Block3-0</code>和<code>Block3-1</code>占据整个32个量化<code>qs</code>对象的最高<strong>4bit</strong>。剩余的2bit位，使用<code>qh</code>数组对象保存，并由相邻两个<strong>Super-Block</strong>的浮点数填满对应的2bit位。</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/cma32uSXkh6dEoL.png" alt="Q6_K" /><figcaption aria-hidden="true">Q6_K</figcaption>
</figure>
<h4 id="quant-error-1">Quant Error</h4>
<p>为了最小化整个数据块的量化误差，可以将某个数据的量化误差表示为： <span class="math display">\[Q(i)_{err}=|q_{i}\times\delta-f_i|\]</span> 或者 <span class="math display">\[Q(i)_{err}=(q_{i}\times\delta-f_i)^2\]</span></p>
<p>则整个<strong>Super-Block</strong>内的量化误差表示为：</p>
<p><span class="math display">\[Q_{err}=\sum_{i=1}^{n}Q(i)_{err}\]</span></p>
<p>由于每个数据对量化误差的敏感度不一，则考虑误差权重的表达式如下：</p>
<p><span class="math display">\[Q_{err}=\sum_{i=1}^{n}W_{i}\times Q(i)_{err}\]</span></p>
<h4 id="quant-scale-opt-1">Quant Scale OPT</h4>
<p>为了<code>最小化</code><strong>Super-Block</strong>内的量化误差之和，则设定优化目标可以表示为：</p>
<p><span class="math display">\[\mathbf{argmin}_{\delta} \quad Q_{err}=\sum_{i=1}^{n}W_i \times (q_{i}\times\delta-f_i)^2\]</span></p>
<p>可以通过求解<code>最小二乘</code>问题，求得优化的 <span class="math inline">\(\delta\)</span>。</p>
<ol type="1">
<li>对 <span class="math inline">\(\delta\)</span>求偏导，并令等式为<code>0</code>得：</li>
</ol>
<p><span class="math display">\[\sum_{i=1}^{n}W_i \times (q_{i}\times\delta-f_i)q_i=0\]</span> 上式展开得 <span class="math display">\[\delta\sum_{i=1}^{n}W_{i}q_{i}^2 =\sum_{i=1}^{n}W_{i}f_iq_i\]</span> 求解得 <span class="math display">\[\delta_{b}=\frac{\sum_{i=1}^{n}W_{i}f_iq_i}{\sum_{i=1}^{n}W_{i}q_{i}^2} \tag{7}\]</span></p>
<h4 id="quant-scale-update-1">Quant Scale Update</h4>
<p>根据量化公式 <span class="math inline">\(q=\frac{f}{\delta}\)</span> 知，通过动态调整量化系数 <span class="math inline">\(\delta\)</span> 的大小，可以对数据的量化结果进行调整。 <span class="math display">\[\set{\frac{1}{\delta_j}|\frac{-\large(2^{bit-1}+\delta_r*j\large)}{|f|_{max}}, j \in[1,step]}\]</span> 假设采用<code>6bit</code>量化， <span class="math inline">\(\delta_r=0.1\)</span>， <span class="math inline">\(step=[-9,9]\)</span>，则</p>
<p><span class="math display">\[\set{\frac{1}{\delta_j}|\frac{-(32+0.1*j)}{|f|_{max}}, j \in[-9,9]}\]</span></p>
<p>从而得到一组新的量化数值 <span class="math inline">\(\set{q_i|i\in[1,n]}\)</span> ，根据量化的最优目标 <span class="math inline">\(\mathtt{minimine} \quad Q_{err}\)</span> ，并结合<code>公式(7)</code>可以得到新的量化系数 <span class="math inline">\(\delta_{b}\)</span> ，并重新计算出新的 <span class="math inline">\(Q_{err}^j=\sum_{i=1}^{n}W_i \times (q_{i}\times\delta_{b}-f_i)^2\)</span> 。 <span class="math display">\[
\begin{align}
Q_{err}^j
&amp;=\sum_{i=1}^{n}W_i \times (q_{i}\delta_{b}-f_i)^2\\
&amp;=\sum_{i=1}^{n}W_if_i^2 -2\sum_{i=1}^{n}W_iq_{i}\delta_{b}f_i + \sum_{i=1}^{n}W_i(q_{i}\delta_{b})^2\\
&amp;=\textcolor{red}{\sum_{i=1}^{n}W_if_i^2}-2\delta_{b}\sum_{i=1}^{n}W_iq_{i}f_i + \delta_{b}^2\sum_{i=1}^{n}W_iq_{i}^2
\end{align}
\tag{8}
\]</span> 其中， 因为校验集数据集在量化过程中是固定的，则 <span class="math inline">\(\sum_{i=1}^{n}W_if_i^2\)</span> 项可以看作常数，将公式(7)计算的 <span class="math inline">\(\delta_b\)</span> 项代入公式(8)可得 <span class="math display">\[
\begin{align}
Q_{err}^j
&amp;=\textcolor{red}{Constant}-2\frac{\sum_{i=1}^{n}W_{i}f_iq_i}{\sum_{i=1}^{n}W_{i}q_{i}^2}\sum_{i=1}^{n}W_iq_{i}f_i + (\frac{\sum_{i=1}^{n}W_{i}f_iq_i}{\sum_{i=1}^{n}W_{i}q_{i}^2})^2\sum_{i=1}^{n}W_iq_{i}^2\\
&amp;=\textcolor{red}{Constant}-\frac{(\sum_{i=1}^{n}W_{i}f_iq_i)^2}{\sum_{i=1}^{n}W_{i}q_{i}^2}
\end{align}
\tag{9}
\]</span> 故 <span class="math inline">\(\mathtt{minimine} \quad Q_{err}^j\)</span> 等效于 <span class="math display">\[\mathtt{maxmine} \quad \frac{(\sum_{i=1}^{n}W_{i}f_iq_i)^2}{\sum_{i=1}^{n}W_{i}q_{i}^2}\]</span> 结合公式(7)可得 <span class="math display">\[\mathtt{maxmine} \quad \delta_{b}\sum_{i=1}^{n}W_{i}f_iq_i\]</span> 只要 <span class="math inline">\(\delta_{b}\sum_{i=1}^{n}W_{i}f_i\hat{q}_i &gt; \delta\sum_{i=1}^{n}W_{i}f_iq_i\)</span> ，则更新 <span class="math inline">\(\delta_{b}\)</span> 为最终输出的量化系数。</p>
<h3 id="symmetric-quant-and-local-optimumq3_k">Symmetric Quant and Local Optimum(Q3_K)</h3>
<p>如下图所示，是<strong>对称量化</strong>算法的数据结构示意图，其中图的左边表示浮点数据的排布，右边的<code>qs</code>和<code>qh</code>表示量化数据的排布。下图是 <code>Q3_K</code> 量化算法的数据存储示意图，整个 <strong>Block</strong> 的<strong>量化系数</strong>以<code>float16</code>格式存储，占据<code>2</code>字节的空间。然后再将整个浮点数据块 <strong>Block</strong> 划分为<code>16</code>个 <strong>Super-Block</strong> ，每个 <strong>Super-Block</strong> 拥有自己的<strong>量化系数</strong>，且各自的数据位宽为<code>6bit</code>，一共占据<code>12</code>个字节空间，具体的量化系数排布如图中右下角所示。量化数值为3bit位宽，其中<strong>低2bit</strong>存储在<code>qs</code>对象中，<strong>高1bit</strong>存储在<code>qh</code>对象中。例如，<code>Block0</code>和<code>Block1</code>都对应<code>qs</code>数组对象的最低2bit位，Block0对应前16个<code>qs</code>数组对象，Block1对应后16个<code>qs</code>数组对象。<code>Block0</code>和<code>Block1</code>同时对应<code>qh</code>数组对象的最低1bit位，Block0对应前16个<code>qh</code>数组对象，Block1对应后16个<code>qh</code>数组对象。</p>
<figure>
<img data-src="https://s2.loli.net/2025/07/06/RiPwFv5JKeqmUVX.png" alt="Q3_K" /><figcaption aria-hidden="true">Q3_K</figcaption>
</figure>
<p>为了最小化<code>Super-Block</code>内的量化误差之和，则设定<strong>RMSE</strong>优化的目标可以表示为：</p>
<p><span class="math display">\[\mathtt{minimine} \quad \sum_{i=0}^{n-1}W_i \times (q_{i}\times\delta-f_i)^2\]</span></p>
<p>可以考虑估计一个局部最优的比例因子，使得量化误差局部最优。</p>
<p>当关注优化单个元素 <span class="math inline">\(q_i\)</span> 时，假设其它元素的量化值保持不变。为了决定 <span class="math inline">\(q_i\)</span> 一个好的候选值，需要一个临时、局部的比例因子，这个比例因子在当前迭代状态下，对于除了元素 <code>i</code> 之外的所有元素来说是“最优”的。</p>
<ul>
<li>考虑排除元素 i 后的加权平方误差：</li>
</ul>
<p><span class="math display">\[Q_{err}(\delta)=\large\sum_{j!=i}^{n-1}W_j \times (q_{j}\times\delta-f_j)^2\]</span></p>
<p>目标是找到一个比例因子 <span class="math inline">\(\delta_{local}\)</span> ，使得这个 <span class="math inline">\(Q_{err}(\delta)\)</span> 最小。 为了找到最小值，我们可以对 <span class="math inline">\(Q_{err}(\delta)\)</span> 关于 <span class="math inline">\(\delta_{local}\)</span> 求导并令导数为<code>零</code>得 <span class="math display">\[\large\delta_{local}=\frac{\sum_{j!=i}^{n-1}W_{j}f_jq_j}{\sum_{j!=i}^{n-1}W_{j}q_{j}^2}=\frac{\sum_{j=0}^{n-1}W_{j}f_jq_j-W_{i}f_iq_i}{\sum_{j=0}^{n-1}W_{j}q_{j}^2-W_{i}q_{i}^2}\]</span></p>
<p>当前量化状态下，排除元素 <code>i</code> 后剩余元素的加权平方误差最小的比例因子。</p>
<h2 id="super-block-scale-quant">Super-Block Scale Quant</h2>
<p>然后对每个 <strong>Super-Block</strong> 的量化系数再进行非对称或对称6bit量化，并将量化系数存储在外部的数据结构中。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="Henry 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Henry 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/llama-cpp/" rel="tag"># llama.cpp</a>
              <a href="/tags/llm-quant/" rel="tag"># llm quant</a>
              <a href="/tags/k-quant/" rel="tag"># k-quant</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/02/22/llama-cpp-kv-cache-manage/" rel="prev" title="KV Cache管理-llama.cpp">
                  <i class="fa fa-angle-left"></i> KV Cache管理-llama.cpp
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-gitalk">gitalk</a></li>
            <li class="tab"><a href="#comment-changyan">changyan</a></li>
            <li class="tab"><a href="#comment-disqus">disqus</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments gitalk-container"></div>
            </div>
            <div class="tab-pane changyan" id="comment-changyan">
              <div class="comments" id="SOHUCS" sid="135bae82957fc0a71f1f0f3f2e6aff0e"></div>
            </div>
            <div class="tab-pane disqus" id="comment-disqus">
              
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
            </div>
        </div>
      </div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2020 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Henry</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3pnaDU1MQ==" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://zgh551.github.io/2025/07/05/llama-cpp-k-quant/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zgh551","repo":"hexo_gittalk","client_id":"1fb8d150a53497be045f","client_secret":"04fcd95e4d714c51c62222db387f2597d4b5a968","admin_user":"zgh551","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"135bae82957fc0a71f1f0f3f2e6aff0e"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>
<script class="next-config" data-name="changyan" type="application/json">{"enable":true,"appid":"cyxCTcyif","appkey":"79391f9a8a4f1f9ddf64d58d44069762","count":true}</script>
<script src="/js/third-party/comments/changyan.js"></script>
<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"https-zgh551-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
